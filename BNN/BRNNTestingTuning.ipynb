{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "BRNNTestingTuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGDwm_kT5VQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from load import sort_and_order, skip_row, write_3d, skip_row, transform, fit_scale, roll, sort_links, tod_interval, split_df_with_val, tilted_loss_np_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hl_3bpv3Uc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "if tf.__version__ != '2.0.0-rc1':\n",
        "  !pip install tensorflow-gpu==2.0.0-rc1\n",
        "  !pip install tensorflow_probability==0.8.0-rc0\n",
        "  !pip install numpy==1.17.2\n",
        "\n",
        "  import os\n",
        "  os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADLeD5Su3OHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import math\n",
        "from datetime import datetime\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhmgIx_o3OHr",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow and numpy versions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXHLmMhB3OHs",
        "colab_type": "code",
        "outputId": "76df6c68-9645-4e33-fbf1-957701bca404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Tensorflow Probability Version .{}\".format(tfp.__version__))\n",
        "print(\"Tensorflow Version .{}\".format(tf.__version__))\n",
        "print(\"Numpy Version .{}\".format(np.__version__))\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow Probability Version .0.8.0-rc0\n",
            "Tensorflow Version .2.0.0-rc1\n",
            "Numpy Version .1.17.2\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BidYwAO0bdc6",
        "colab_type": "code",
        "outputId": "6c656a2b-9c0b-4a13-98fb-fd9bd7e0a364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/andersparslov/BRNN.git\n",
        "import os\n",
        "os.chdir('BRNN')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BRNN'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 25 (delta 11), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzQK5WG03OHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(lags, start_train, end_train, end_test):\n",
        "  data = pd.read_csv('link_travel_time_local.csv.gz', compression='gzip', parse_dates = True, index_col = 0)\n",
        "\n",
        "  ## Sort links by order \n",
        "  data, order = sort_links(data, '1973:1412', '7057:7058')\n",
        "  ## Make a link order column e.g here the neighbouring links for link 1 are 0 and 2.\n",
        "  data['link_order'] = data['link_ref'].astype('category')\n",
        "  not_in_list = data['link_order'].cat.categories.difference(order)\n",
        "  data['link_order'] = data['link_order'].cat.set_categories(np.hstack((order, not_in_list)), ordered=True)\n",
        "  data['link_order'] = data['link_order'].cat.codes\n",
        "  ## Add week of day column [Monday, ..., Sunday] = [0, ..., 6]\n",
        "  data['Weekday'] = data.index.weekday\n",
        "  ## Add hour of the time to dataframe\n",
        "  data['Hour'] = data.index.hour\n",
        "  ## Add time of day variables to data frame\n",
        "  data['TOD'] = data.Hour.apply(tod_interval)\n",
        "  data = data.sort_values('link_order')\n",
        "  data_train, data_test = split_df(data, start_train = start_train, end_train = end_train, end_test = end_test)\n",
        "\n",
        "  ## Transform train and test set using the mean and std for train set.\n",
        "  means_df_train, scales_df_train = fit_scale(data_train, order)\n",
        "  ts_train_df, mean_train_df, scale_train_df = transform(data_train, \n",
        "                                                    means_df_train, \n",
        "                                                    scales_df_train, \n",
        "                                                    order,\n",
        "                                                    freq = '15min')\n",
        "  ts_test_df, mean_test_df, scale_test_df = transform(data_test, \n",
        "                                                  means_df_train, \n",
        "                                                  scales_df_train, \n",
        "                                                  order,\n",
        "                                                  freq = '15min')\n",
        "  return ts_train_df, mean_train_df, scale_train_df, ts_test_df, mean_test_df, scale_test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR_JlHRZ3OH6",
        "colab_type": "text"
      },
      "source": [
        "# Mixture Prior\n",
        "$$P(\\boldsymbol{w}) = \\prod_j \\left( \\pi \\mathcal{N}\\left(w_j | 0, \\sigma_1^2\\right) + (1-\\pi) \\mathcal{N}\\left(w_j | 0, \\sigma_2^2\\right) \\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOzV8sCV3OH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MixturePrior(object):\n",
        "    def __init__(self, pi, sigma1, sigma2):\n",
        "        self.mu, self.pi, self.sigma1, self.sigma2 = (np.float32(v) for v in (0.0, pi, sigma1, sigma2))\n",
        "        self.dist = tfd.MixtureSameFamily(\n",
        "                  mixture_distribution=tfd.Categorical(\n",
        "                    probs=[1-self.pi, self.pi]),\n",
        "                    components_distribution=tfd.Normal(\n",
        "                      loc=[0, 0],       \n",
        "                      scale=[self.sigma1, self.sigma2]))\n",
        "        \n",
        "    def sample(self):\n",
        "      return self.dist.sample()\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        return self.dist.log_prob(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvnwqJE-GAPr",
        "colab_type": "text"
      },
      "source": [
        "## Variational Posterior "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHvYHY4P3OH_",
        "colab_type": "text"
      },
      "source": [
        "$$q(\\boldsymbol{w} | \\mu, \\sigma) = \\mathcal{N}(w | \\mu, \\sigma^2), \\enspace \\sigma = \\log(1 + \\exp(\\rho))$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOuLagCk3OIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalPosterior(object):\n",
        "    def __init__(self, mu, rho):\n",
        "        super().__init__()\n",
        "        self.mu = mu\n",
        "        self.rho = rho\n",
        "        self.stdNorm = tfd.Normal(0,1)\n",
        "    \n",
        "    @property\n",
        "    def sigma(self):\n",
        "        return tf.math.softplus(self.rho)\n",
        "    \n",
        "    def sample(self, training, sampling=True):\n",
        "      if training:\n",
        "        epsilon = self.stdNorm.sample(tf.shape(self.rho))\n",
        "        return self.mu + self.sigma*epsilon\n",
        "      elif sampling:\n",
        "        return tfd.Normal(self.mu, self.sigma).sample()\n",
        "      else:\n",
        "        return self.mu\n",
        "    \n",
        "    def log_prob(self, x):\n",
        "        return tf.reduce_sum(-tf.math.log(tf.math.sqrt(2 * math.pi))\n",
        "                - tf.math.log(self.sigma)\n",
        "                - ((x - self.mu) ** 2) / (2 * self.sigma ** 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXE_q4fx3OID",
        "colab_type": "text"
      },
      "source": [
        "<a id='basic_model'></a>\n",
        "# Basic (Vanilla) Cell\n",
        "\n",
        "[Click to get to untied cell](#untied_model)\n",
        "\n",
        "[Click to get to tied cell](#tied_model)\n",
        "\n",
        "[Click to get to rnn network](#rnn)\n",
        "\n",
        "[Click to get to model training](#training)\n",
        "\n",
        "[Click to get to model testing](#testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdeyCFtx3OIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_all = tf.math.reduce_sum\n",
        "class MinimalRNNCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, training, init, prior, **kwargs):\n",
        "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
        "        self.init = init\n",
        "        self.is_training = training\n",
        "        self.units = units\n",
        "        self.state_size = units\n",
        "        self.prior = prior\n",
        "        \n",
        "    def initialise_cell(self, links):\n",
        "        self.W_mu = self.add_weight(shape=(links, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='W_mu', trainable=True)\n",
        "        self.W_rho = self.add_weight(shape=(links, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='W_rho', trainable=True)\n",
        "        self.U_mu = self.add_weight(shape=(self.units, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='U_mu', trainable=True)\n",
        "        self.U_rho = self.add_weight(shape=(self.units, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='U_rho', trainable=True)\n",
        "        self.B_mu = self.add_weight(shape=(1,self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='B_mu', trainable=True)\n",
        "        self.B_rho = self.add_weight(shape=(1,self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='B_rho', trainable=True)\n",
        "        \n",
        "        ## Make sure following is only printed once during training and not for testing!\n",
        "        print(\"  Basic cell has been built (in:\", links, \") (out:\", self.units, \")\")\n",
        "        self.W_dist = VariationalPosterior(self.W_mu, self.W_rho)\n",
        "        self.U_dist = VariationalPosterior(self.U_mu, self.U_rho)\n",
        "        self.B_dist = VariationalPosterior(self.B_mu, self.B_rho)\n",
        "        self.sampling = False\n",
        "        self.built = True\n",
        "    \n",
        "    def call(self, inputs, states):\n",
        "        self.W = self.W_dist.sample(self.is_training, self.sampling)\n",
        "        self.U = self.U_dist.sample(self.is_training, self.sampling)\n",
        "        self.B = self.B_dist.sample(self.is_training, self.sampling)\n",
        "        if self.is_training:\n",
        "            self.log_prior = sum_all(self.prior.log_prob(self.B)) + sum_all(self.prior.log_prob(self.W)) + sum_all(self.prior.log_prob(self.U)) \n",
        "            self.log_variational_posterior  = sum_all(self.W_dist.log_prob(self.W))\n",
        "            self.log_variational_posterior += sum_all(self.U_dist.log_prob(self.U))\n",
        "            self.log_variational_posterior += sum_all(self.B_dist.log_prob(self.B))\n",
        "        h = tf.linalg.matmul(inputs, self.W)\n",
        "        output = tf.math.tanh(self.B + h + tf.linalg.matmul(states[0], self.U))\n",
        "        return output, [output]\n",
        "\n",
        "    def get_initial_state(self, inputs = None, batch_size = None, dtype = None):\n",
        "        return [tf.zeros((batch_size, self.state_size), dtype = dtype)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNuLp6DW3OII",
        "colab_type": "text"
      },
      "source": [
        "<a id='untied_model'></a>\n",
        "# Untied Weights LSTM Cell\n",
        "[Click to get back to basic cell](#basic_model)\n",
        "\n",
        "[Click to get to tied cell](#tied_model)\n",
        "\n",
        "[Click to get to rnn network](#rnn)\n",
        "\n",
        "[Click to get to model training](#training)\n",
        "\n",
        "[Click to get to model testing](#testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNnsJsRQ3OIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianLSTMCell_Untied(tf.keras.Model):\n",
        "    def __init__(self, num_units, training, init, prior, **kwargs):\n",
        "        super(BayesianLSTMCell_Untied, self).__init__(num_units, **kwargs)\n",
        "        self.init = init\n",
        "        self.units = num_units\n",
        "        self.is_training = training\n",
        "        self.state_size = self.units\n",
        "        self.prior = prior\n",
        "        \n",
        "    def initialise_cell(self, links):\n",
        "        self.num_links = links\n",
        "        self.Ui_mu = self.add_weight(shape=(self.units, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Ui_mu', trainable=True)\n",
        "        self.Ui_rho = self.add_weight(shape=(self.units, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Ui_rho', trainable=True)\n",
        "        self.Uo_mu = self.add_weight(shape=(self.units, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Uo_mu', trainable=True)\n",
        "        self.Uo_rho = self.add_weight(shape=(self.units, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Uo_rho', trainable=True)\n",
        "        self.Uf_mu = self.add_weight(shape=(self.units, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Uf_mu', trainable=True)\n",
        "        self.Uf_rho = self.add_weight(shape=(self.units, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Uf_rho', trainable=True)\n",
        "        self.Ug_mu = self.add_weight(shape=(self.units, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Ug_mu', trainable=True)\n",
        "        self.Ug_rho = self.add_weight(shape=(self.units, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Ug_rho', trainable=True)\n",
        "        \n",
        "        self.Wi_mu = self.add_weight(shape=(self.num_links, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Wi_mu', trainable=True)\n",
        "        self.Wi_rho = self.add_weight(shape=(self.num_links, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Wi_rho', trainable=True)\n",
        "        self.Wo_mu = self.add_weight(shape=(self.num_links, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Wo_mu', trainable=True)\n",
        "        self.Wo_rho = self.add_weight(shape=(self.num_links, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Wo_rho', trainable=True)\n",
        "        self.Wf_mu = self.add_weight(shape=(self.num_links, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Wf_mu', trainable=True)\n",
        "        self.Wf_rho = self.add_weight(shape=(self.num_links, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Wf_rho', trainable=True)\n",
        "        self.Wg_mu = self.add_weight(shape=(self.num_links, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Wg_mu', trainable=True)\n",
        "        self.Wg_rho = self.add_weight(shape=(self.num_links, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Wg_rho', trainable=True)\n",
        "        \n",
        "        self.Bi_mu = self.add_weight(shape=(1, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Wi_mu', trainable=True)\n",
        "        self.Bi_rho = self.add_weight(shape=(1, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Wi_rho', trainable=True)\n",
        "        self.Bo_mu = self.add_weight(shape=(1, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Wo_mu', trainable=True)\n",
        "        self.Bo_rho = self.add_weight(shape=(1, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Wo_rho', trainable=True)\n",
        "        self.Bf_mu = self.add_weight(shape=(1, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Wf_mu', trainable=True)\n",
        "        self.Bf_rho = self.add_weight(shape=(1, self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='Wf_rho', trainable=True)\n",
        "        self.Bg_mu = self.add_weight(shape=(1, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Wg_mu', trainable=True)\n",
        "        self.Bg_rho = self.add_weight(shape=(1, self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='Wg_rho', trainable=True)\n",
        "        \n",
        "        self.Ui_dist = VariationalPosterior(self.Ui_mu, self.Ui_rho)\n",
        "        self.Uo_dist = VariationalPosterior(self.Uo_mu, self.Uo_rho)\n",
        "        self.Uf_dist = VariationalPosterior(self.Uf_mu, self.Uf_rho)\n",
        "        self.Ug_dist = VariationalPosterior(self.Ug_mu, self.Ug_rho)\n",
        "        self.Wi_dist = VariationalPosterior(self.Wi_mu, self.Wi_rho)\n",
        "        self.Wo_dist = VariationalPosterior(self.Wo_mu, self.Wo_rho)\n",
        "        self.Wf_dist = VariationalPosterior(self.Wf_mu, self.Wf_rho)\n",
        "        self.Wg_dist = VariationalPosterior(self.Wg_mu, self.Wg_rho)\n",
        "        self.Bi_dist = VariationalPosterior(self.Bi_mu, self.Bi_rho)\n",
        "        self.Bo_dist = VariationalPosterior(self.Bo_mu, self.Bo_rho)\n",
        "        self.Bf_dist = VariationalPosterior(self.Bf_mu, self.Bf_rho)\n",
        "        self.Bg_dist = VariationalPosterior(self.Bg_mu, self.Bg_rho)\n",
        "        ## Make sure following is only printed once during training and not for testing!\n",
        "        print(\"  Untied cell has been built (in:\", links, \") (out:\", self.units, \")\")\n",
        "        self.sampling = False\n",
        "        self.built = True\n",
        "    \n",
        "    def call(self, inputs, states):\n",
        "        Ui = self.Ui_dist.sample(self.is_training, self.sampling)\n",
        "        Uo = self.Uo_dist.sample(self.is_training, self.sampling)\n",
        "        Uf = self.Uf_dist.sample(self.is_training, self.sampling)\n",
        "        Ug = self.Ug_dist.sample(self.is_training, self.sampling)\n",
        "        Wi = self.Wi_dist.sample(self.is_training, self.sampling)\n",
        "        Wo = self.Wo_dist.sample(self.is_training, self.sampling)\n",
        "        Wf = self.Wf_dist.sample(self.is_training, self.sampling)\n",
        "        Wg = self.Wg_dist.sample(self.is_training, self.sampling)\n",
        "        Bi = self.Bi_dist.sample(self.is_training, self.sampling)\n",
        "        Bo = self.Bo_dist.sample(self.is_training, self.sampling)\n",
        "        Bf = self.Bf_dist.sample(self.is_training, self.sampling)\n",
        "        Bg = self.Bg_dist.sample(self.is_training, self.sampling)\n",
        "\n",
        "        c_t, h_t = tf.split(value=states[0], num_or_size_splits=2, axis=0)\n",
        "        \n",
        "        inputs = tf.cast(inputs, tf.float32)\n",
        "        i = tf.sigmoid(Bi + tf.linalg.matmul(h_t, Ui) + tf.linalg.matmul(inputs, Wi))\n",
        "        o = tf.sigmoid(Bo + tf.linalg.matmul(h_t, Uo) + tf.linalg.matmul(inputs, Wo))\n",
        "        f = tf.sigmoid(Bf + tf.linalg.matmul(h_t, Uf) + tf.linalg.matmul(inputs, Wf))\n",
        "        g = tf.math.tanh(Bg + tf.linalg.matmul(h_t, Ug) + tf.linalg.matmul(inputs, Wg))\n",
        "        \n",
        "        self.log_prior  =  sum_all(self.prior.log_prob(Ui) + self.prior.log_prob(Uo) + self.prior.log_prob(Uf) + self.prior.log_prob(Ug))\n",
        "        self.log_prior +=  sum_all(self.prior.log_prob(Wi) + self.prior.log_prob(Wo) + self.prior.log_prob(Wf) + self.prior.log_prob(Wg))\n",
        "        self.log_prior +=  sum_all(self.prior.log_prob(Bi) + self.prior.log_prob(Bo) + self.prior.log_prob(Bf) + self.prior.log_prob(Bg))\n",
        "        self.log_variational_posterior  = sum_all(self.Ui_dist.log_prob(Ui) + self.Uo_dist.log_prob(Uo) + self.Uf_dist.log_prob(Uf) + self.Ug_dist.log_prob(Ug))\n",
        "        self.log_variational_posterior += sum_all(self.Wi_dist.log_prob(Wi) + self.Wo_dist.log_prob(Wo) + self.Wf_dist.log_prob(Wf) + self.Wg_dist.log_prob(Wg))\n",
        "        self.log_variational_posterior += sum_all(self.Bi_dist.log_prob(Bi) + self.Bo_dist.log_prob(Bo) + self.Bf_dist.log_prob(Bf) + self.Bg_dist.log_prob(Bg))\n",
        "        \n",
        "        c_new = f*c_t + i*g\n",
        "        h_new = o*tf.math.tanh(c_new)\n",
        "        new_state = tf.concat([c_new, h_new], axis=0)\n",
        "        return h_new, new_state\n",
        "    \n",
        "    def get_initial_state(self, inputs = None, batch_size = None, dtype = None):\n",
        "        return tf.zeros((2*batch_size, self.units), dtype = dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-02vC9b3OIN",
        "colab_type": "text"
      },
      "source": [
        "<a id='tied_model'></a>\n",
        "# Tied Weights LSTM Cell\n",
        "[Click to get back to basic cell](#basic_model)\n",
        "\n",
        "[Click to get back to untied cell](#untied_model)\n",
        "\n",
        "[Click to get to rnn network](#rnn)\n",
        "\n",
        "[Click to get to model training](#training)\n",
        "\n",
        "[Click to get to model testing](#testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3bZvXEk3OIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianLSTMCellTied(tf.keras.Model):\n",
        "    def __init__(self, num_units, training, init, prior, **kwargs):\n",
        "        super(BayesianLSTMCellTied, self).__init__(num_units, **kwargs)\n",
        "        self.init = init\n",
        "        self.prior = prior \n",
        "        self.units = num_units\n",
        "        self.state_size = num_units\n",
        "        self.is_training = training\n",
        "        \n",
        "    def initialise_cell(self, links):\n",
        "        self.num_links = links\n",
        "        self.W_mu = self.add_weight(shape=(self.units+self.num_links, 4*self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='W_mu', trainable=True)\n",
        "        self.W_rho = self.add_weight(shape=(self.units+self.num_links, 4*self.units),\n",
        "                                      initializer=self.init,\n",
        "                                      name='W_rho', trainable=True)\n",
        "        self.B_mu = self.add_weight(shape=(1, 4*self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='B_mu', trainable=True)\n",
        "        self.B_rho = self.add_weight(shape=(1, 4*self.units),\n",
        "                                    initializer=self.init,\n",
        "                                    name='B_rho', trainable=True)\n",
        "        \n",
        "        self.W_dist = VariationalPosterior(self.W_mu, self.W_rho)\n",
        "        self.B_dist = VariationalPosterior(self.B_mu, self.B_rho)\n",
        "        ## Make sure following is only printed once during training and not for testing!\n",
        "        print(\"  Tied Cell has been built (in:\", links, \") (out:\", self.units, \")\")\n",
        "        self.sampling = False\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        W = self.W_dist.sample(self.is_training, self.sampling)\n",
        "        B = self.B_dist.sample(self.is_training, self.sampling)\n",
        "        c_t, h_t = tf.split(value=states[0], num_or_size_splits=2, axis=0)\n",
        "        concat_inputs_hidden = tf.concat([tf.cast(inputs, tf.float32), h_t], 1)\n",
        "        concat_inputs_hidden = tf.nn.bias_add(tf.matmul(concat_inputs_hidden, tf.squeeze(W)), \n",
        "                                              tf.squeeze(B))\n",
        "        \n",
        "        self.log_prior =  sum_all(self.prior.log_prob(W)) + sum_all(self.prior.log_prob(B))\n",
        "        self.log_variational_posterior = sum_all(self.W_dist.log_prob(W)) + sum_all(self.B_dist.log_prob(B))\n",
        "        \n",
        "        # Gates: Input, New, Forget and Output\n",
        "        i, j, f, o = tf.split(value = concat_inputs_hidden, num_or_size_splits = 4, axis = 1)\n",
        "        c_new = c_t*tf.sigmoid(f) + tf.sigmoid(i)*tf.math.tanh(j)\n",
        "        h_new = tf.math.tanh(c_new)*tf.sigmoid(o)\n",
        "        new_state = tf.concat([c_new, h_new], axis=0)\n",
        "        return h_new, new_state\n",
        "    \n",
        "    def get_initial_state(self, inputs = None, batch_size = None, dtype = None):\n",
        "        return tf.zeros((2*batch_size, self.units), dtype = dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9EI5qwP3OIT",
        "colab_type": "text"
      },
      "source": [
        "<a id='rnn'></a>\n",
        "# RNN Network \n",
        "[Click to get back to basic cell](#basic_model)\n",
        "\n",
        "[Click to get back to untied cell](#untied_model)\n",
        "\n",
        "[Click to get to tied cell](#tied_model)\n",
        "\n",
        "[Click to get to model training](#training)\n",
        "\n",
        "[Click to get to model testing](#testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkcsdYdh3OIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianRNN(tf.keras.Model):\n",
        "    def __init__(self, num_units, num_links, batch_size, init, cell_type, prior, **kwargs):\n",
        "        super(BayesianRNN, self).__init__(**kwargs)\n",
        "        self.cell_type = cell_type\n",
        "        self.init = init\n",
        "        self.num_units_lst = num_units\n",
        "        self.num_links = num_links\n",
        "        self.batch_size = batch_size\n",
        "        self.cell_prior = prior\n",
        "        self.prior = prior\n",
        "        self.build()\n",
        "    \n",
        "    def build(self):\n",
        "        print(\"Building net...\")\n",
        "        self.cell_lst = []\n",
        "        state_size = self.num_links\n",
        "        for i, num_units in enumerate(self.num_units_lst):\n",
        "          if self.cell_type == 'Basic':\n",
        "              self.cell_lst.append(MinimalRNNCell(num_units, training=True, init=self.init, prior=self.cell_prior))\n",
        "          elif self.cell_type == 'TiedLSTM':\n",
        "              self.cell_lst.append(BayesianLSTMCellTied(num_units, training=True, init=self.init, prior=self.cell_prior))\n",
        "          else:\n",
        "              self.cell_lst.append(BayesianLSTMCell_Untied(num_units, training=True, init=self.init, prior=self.cell_prior))\n",
        "          self.cell_lst[-1].initialise_cell(state_size)\n",
        "          state_size = num_units\n",
        "            \n",
        "        self.weight_mu = self.add_weight(shape=(self.num_units_lst[-1],self.num_links),\n",
        "                                 initializer=self.init,\n",
        "                                 name='weight_mu')\n",
        "        self.weight_rho = self.add_weight(shape=(self.num_units_lst[-1],self.num_links),\n",
        "                                 initializer=self.init,\n",
        "                                 name='weight_mu')\n",
        "        self.bias_mu = self.add_weight(shape=(self.num_links,),\n",
        "                                     initializer=self.init,\n",
        "                                     name='bias_mu', trainable=True)\n",
        "        self.bias_rho = self.add_weight(shape=(self.num_links,),\n",
        "                                     initializer=self.init,\n",
        "                                     name='bias_mu', trainable=True)\n",
        "        self.weight_dist = VariationalPosterior(self.weight_mu, self.weight_rho) \n",
        "        self.bias_dist = VariationalPosterior(self.bias_mu, self.bias_rho)     \n",
        "        print(\"  Output layer has been built (in:\", self.num_units_lst[-1], \") (out:\", 1, \")\")\n",
        "\n",
        "        ## The diagonal of the correlation matrix\n",
        "        self.scale_prior = tfd.LKJ(dimension=self.num_links, concentration=10, input_output_cholesky=True)\n",
        "        self.y_rho = self.add_weight(shape=(self.num_links*((self.num_links-1)/2 + 1),), \n",
        "                                     initializer='zeros',\n",
        "                                     name='y_rho',\n",
        "                                     trainable=True)\n",
        "        self.built = True\n",
        "    @property\n",
        "    def y_std(self):\n",
        "        cor = tfb.ScaleTriL(diag_bijector=tfb.Softplus(),\n",
        "                            diag_shift=None)\n",
        "        return cor.forward(self.y_rho)\n",
        "\n",
        "    def call(self, batch_x, training, sampling):\n",
        "        self.weight = self.weight_dist.sample(training, sampling)\n",
        "        self.bias = self.bias_dist.sample(training, sampling)\n",
        "        if training:\n",
        "            self.log_prior_dense = sum_all(self.prior.log_prob(self.weight)) + sum_all(self.prior.log_prob(self.bias))\n",
        "            self.log_variational_posterior_dense  = self.weight_dist.log_prob(self.weight) \n",
        "            self.log_variational_posterior_dense += self.bias_dist.log_prob(self.bias)\n",
        "        for cell in self.cell_lst:\n",
        "          cell.is_training = training\n",
        "          cell.sampling = sampling\n",
        "\n",
        "        inputs = tf.convert_to_tensor(batch_x)\n",
        "        rnn = tf.keras.layers.RNN(self.cell_lst)\n",
        "        ## RNN layer\n",
        "        final_rnn_output = rnn(inputs)\n",
        "        ## Dense layer\n",
        "        self.outputs = tf.linalg.matmul(final_rnn_output, self.weight) + self.bias   \n",
        "        return self.outputs\n",
        "    \n",
        "    def log_prior(self):\n",
        "        return sum(sum_all(cell.log_prior) for cell in self.cell_lst) + sum_all(self.log_prior_dense) + sum_all(self.scale_prior.log_prob(self.y_std))\n",
        "    \n",
        "    def log_variational_posterior(self):\n",
        "        return sum(sum_all(cell.log_variational_posterior) for cell in self.cell_lst) + sum_all(self.log_variational_posterior_dense)\n",
        "    \n",
        "    def elbo(self, batch_x, batch_y, batch_ind, num_batches,  training, sampling=True):\n",
        "        output = self(batch_x, training, sampling)\n",
        "        assert(batch_y.shape[1] == self.num_links)\n",
        "        assert(output.shape == batch_y.shape)\n",
        "        pred_dist = tfd.MultivariateNormalTriL(output, scale_tril=self.y_std)\n",
        "        self.nll = -tf.math.reduce_sum(pred_dist.log_prob(batch_y))\n",
        "        kl_weight = 2**(num_batches - batch_ind) / (2**num_batches - 1)\n",
        "        return (self.log_variational_posterior() - self.log_prior())/num_batches + self.nll, sum_all((output - batch_y)**2) / self.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2efPjKDA3OIZ",
        "colab_type": "text"
      },
      "source": [
        "<a id='training'></a>\n",
        "# Model Training\n",
        "\n",
        "[Click to get back to basic rnn model](#basic_model)\n",
        "\n",
        "[Click to get back to untied model](#untied_model)\n",
        "\n",
        "[Click to get back to tied model](#tied_model)\n",
        "\n",
        "[Click to get to model testing](#testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5HOeJNVxo6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(mod, data_train, lr):\n",
        "    elbo_sum = 0\n",
        "    mse_sum = 0\n",
        "    batch_ind = 1\n",
        "    optimizer = tf.keras.optimizers.Adam(lr = lr)\n",
        "    for batch_x, batch_y in data_train:\n",
        "        x = tf.cast(batch_x, tf.float32)\n",
        "        y = tf.cast(batch_y, tf.float32)\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, mse = mod.elbo(x, y[:,0], batch_ind, num_batch_train, training=True)\n",
        "        gradients = tape.gradient(loss, mod.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, mod.trainable_variables))\n",
        "        batch_ind = batch_ind + 1\n",
        "        elbo_sum += loss\n",
        "        mse_sum += mse\n",
        "    return elbo_sum, mse_sum\n",
        "\n",
        "def val_loss(mod, data_test):\n",
        "    elbo_sum = 0\n",
        "    mse_sum = 0\n",
        "    batch_ind = 1\n",
        "    for batch_x, batch_y in data_test:\n",
        "        x = tf.cast(batch_x, tf.float32)\n",
        "        y = tf.cast(batch_y, tf.float32)\n",
        "        loss, mse = mod.elbo(x, y[:,0], batch_ind, num_batch_test, training=False, sampling=False)\n",
        "        elbo_sum += loss\n",
        "        mse_sum += mse\n",
        "        batch_ind = batch_ind + 1\n",
        "    return elbo_sum, mse_sum\n",
        "\n",
        "def plot_val(mod, data_test):\n",
        "    y_pred = np.zeros((y_test.shape))\n",
        "    T = 0\n",
        "    for x_batch, _ in data_test:\n",
        "        x = tf.cast(x_batch,tf.float32)\n",
        "        y_pred[T*batch_size:(T+1)*batch_size, 0] = mod(x, training=False, sampling=False)\n",
        "        T = T+1\n",
        "    for lnk in range(y_test.shape[2]):\n",
        "        print(\"  Link \", lnk)\n",
        "        plt.plot(y_ix_test, y_test[:,0,lnk], 'bo', alpha = 0.4)\n",
        "        plt.plot(y_ix_test, y_pred[:,0,lnk], c = 'r')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "n294G4T63OId",
        "colab_type": "code",
        "outputId": "2de58739-8304-43b3-d4db-41831b6972d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "sigma1 = 1\n",
        "sigma2 = np.exp(-6)\n",
        "lags = 10\n",
        "preds = 1\n",
        "\n",
        "start_train_lst = ['2019-01-01', '2019-01-07', '2019-01-14', '2019-01-21', '2019-02-01']\n",
        "end_train_lst = ['2019-01-31', '2019-02-07', '2019-02-14', '2019-02-21', '2019-03-01']\n",
        "end_test_lst = ['2019-02-07', '2019-02-14', '2019-02-21', '2019-03-01', '2019-03-07']\n",
        "    \n",
        "num_partitions = 1\n",
        "num_links = 16\n",
        "batch_size = 80\n",
        "\n",
        "cell_types = ['UntiedLSTM']\n",
        "prior = MixturePrior(0.10, sigma1, sigma2)\n",
        "init = 'uniform'\n",
        "\n",
        "lr = 1e-2\n",
        "epochs = 200\n",
        "patience = 10\n",
        "\n",
        "lag_lst = np.arange(2, 40, 4)\n",
        "units = 10\n",
        "quantiles = np.array([0.05, 0.95])\n",
        "\n",
        "mse =  np.empty((num_partitions, len(lag_lst), len(cell_types)))\n",
        "icp =  np.empty((num_partitions, len(lag_lst), len(cell_types)))\n",
        "mil =  np.empty((num_partitions, len(lag_lst), len(cell_types)))\n",
        "time = np.empty((num_partitions, len(lag_lst), len(cell_types)))\n",
        "\n",
        "for l, lags in enumerate(lag_lst):\n",
        "  print(\"Units {}\".format(units))\n",
        "  ## Instantiate nets\n",
        "  nets = []\n",
        "  best_weights = [None, None, None]\n",
        "  for cell in cell_types:\n",
        "    nets.append(BayesianRNN([units], num_links, batch_size, init, cell, prior))\n",
        "\n",
        "  for part in range(num_partitions):\n",
        "    start_train = start_train_lst[part]\n",
        "    end_train = end_train_lst[part]\n",
        "    end_test = end_test_lst[part]\n",
        "    ts_train_df, mean_train_df, scale_train_df, ts_test_df, mean_test_df, scale_test_df = load_data(lags, start_train, end_train, end_test)\n",
        "\n",
        "    X_train, y_train, y_ix_train, y_mean_train, y_std_train = roll(ts_train_df.index, \n",
        "                                                                    ts_train_df.values,\n",
        "                                                                    mean_train_df.values,\n",
        "                                                                    scale_train_df.values,\n",
        "                                                                    lags, \n",
        "                                                                    preds)\n",
        "    X_test, y_test, y_ix_test, y_mean_test, y_std_test = roll(ts_test_df.index, \n",
        "                                                              ts_test_df.values, \n",
        "                                                              mean_test_df.values,\n",
        "                                                              scale_test_df.values,\n",
        "                                                              lags, \n",
        "                                                              preds)\n",
        "    num_batch_train = int(X_train.shape[0]/batch_size)\n",
        "    num_batch_test = int(X_test.shape[0]/batch_size)\n",
        "\n",
        "    data_train = tf.data.Dataset.from_tensor_slices((X_train, \n",
        "                                                    y_train)).shuffle(1000).batch(batch_size, drop_remainder=True)\n",
        "    data_test = tf.data.Dataset.from_tensor_slices((X_test, \n",
        "                                                    y_test)).batch(batch_size, drop_remainder=True)\n",
        "    drop_train = len(y_train) - num_batch_train*batch_size\n",
        "    drop_test = len(y_test) - num_batch_test*batch_size\n",
        "    X_train, y_train, y_ix_train, y_mean_train, y_std_train = drop_remainder(X_train, y_train, y_ix_train, y_mean_train, y_std_train, drop_train)\n",
        "    X_test, y_test, y_ix_test, y_mean_test, y_std_test = drop_remainder(X_test, y_test, y_ix_test, y_mean_test, y_std_test, drop_test)\n",
        "    \n",
        "    for n, net in enumerate(nets):\n",
        "      init_lr = 1e-2\n",
        "      t1 = datetime.now()\n",
        "      ## Initialise weights using last partition\n",
        "      if part != 0:\n",
        "        init_lr = 5e-3\n",
        "        #net.load_weights(\"{}_l{}_p{}.hdf5\".format(cell_types[n], l, part-1))\n",
        "        net.set_weights(best_weights[n])\n",
        "\n",
        "      best_elbo = 10000000000000000000000\n",
        "      best_mse = 10000000000000000000000\n",
        "      lr = init_lr\n",
        "      for epoch in range(epochs):\n",
        "        ## Training\n",
        "        elbo_sum, mse_sum = train_step(net, data_train, lr)\n",
        "        mse_avg_train = mse_sum.numpy() / num_batch_train\n",
        "        elbo_avg = elbo_sum.numpy() / num_batch_train\n",
        "        ## Validation\n",
        "        elbo_test, mse_sum = val_loss(net, data_test)\n",
        "        mse_avg_test = mse_sum.numpy() / num_batch_test\n",
        "        elbo_avg_test = elbo_test.numpy() / num_batch_test\n",
        "        \n",
        "        if elbo_avg_test < best_elbo:\n",
        "            best_weights[n] = net.get_weights()\n",
        "            best_elbo = elbo_avg_test\n",
        "            patience_counter = 0\n",
        "        elif mse_avg_test < best_mse:\n",
        "            best_weights[n] = net.get_weights()\n",
        "            best_mse = mse_avg_test\n",
        "            patience_counter = 0\n",
        "        patience_counter += 1\n",
        "        \n",
        "        if patience_counter > patience:\n",
        "            print(\"Partition {} early stopping after {} epochs\".format(part, epoch))\n",
        "            break\n",
        "        if patience_counter >= int(patience/2):\n",
        "            lr = init_lr*np.exp(-0.1*epoch)\n",
        "      ## Set weights back to the best model\n",
        "      net.set_weights(best_weights[n])\n",
        "      t2 = datetime.now()\n",
        "      time[part, l, n] = (t2-t1).seconds\n",
        "\n",
        "      nsamples = 50\n",
        "      y_pred = np.empty((nsamples, y_test.shape[0],y_test.shape[2]))\n",
        "      T = 0\n",
        "      for x_batch, _ in data_test:\n",
        "          x = tf.cast(x_batch,tf.float32)\n",
        "          for samp in range(nsamples):\n",
        "            out = net(x, training=False, sampling=True)\n",
        "            y_pred[samp, T*batch_size:(T+1)*batch_size] = tfd.MultivariateNormalTriL(out, scale_tril=net.y_std).sample()\n",
        "          T = T+1\n",
        "      y_test_all = np.sum(y_test[:,0], axis=1)\n",
        "\n",
        "      icp_lnks = np.zeros(num_links)\n",
        "      mil_lnks = np.zeros(num_links) \n",
        "      for lnk in range(num_links):\n",
        "        q1 = np.quantile(y_pred[:,:,lnk], quantiles[ 0], axis=0)\n",
        "        q2 = np.quantile(y_pred[:,:,lnk], quantiles[ 1], axis=0)\n",
        "        icp_lnks[lnk] = 1 - (np.sum(y_test[:,0,lnk] < q1) + np.sum(y_test[:,0,lnk] > q2) )/len(y_test)\n",
        "        mil_lnks[lnk] = np.sum(np.maximum(0, q2 - q1)) / len(y_test)\n",
        "      icp[part, l, n] = np.mean(icp_lnks)\n",
        "      mil[part, l, n] = np.mean(mil_lnks)\n",
        "      y_mean = np.sum(np.mean(y_pred, axis=0), axis=1)\n",
        "      mse[part, l, n] = np.sum((y_mean - y_test_all)**2) / len(y_test_all)\n",
        "\n",
        "      print(\"Lags {}/{} partition {} cell {} \".format(lags, max(lag_lst), part, cell_types[n]), end ='')\n",
        "      print(icp[part,l,n], mil[part,l,n], mse[part,l,n])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/categorical.py:225: Categorical._logits_deprecated_behavior (from tensorflow_probability.python.distributions.categorical) is deprecated and will be removed after 2019-10-01.\n",
            "Instructions for updating:\n",
            "The `logits` property will return `None` when the distribution is parameterized with `logits=None`. Use `logits_parameter()` instead.\n",
            "Units 10\n",
            "Building net...\n",
            "  Untied cell has been built (in: 16 ) (out: 10 )\n",
            "  Output layer has been built (in: 10 ) (out: 1 )\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2520: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Partition 0 early stopping after 26 epochs\n",
            "Lags 2/38 partition 0 cell UntiedLSTM 0.9001736111111112 4.004931381989994 85.23801450587867\n",
            "Units 10\n",
            "Building net...\n",
            "  Untied cell has been built (in: 16 ) (out: 10 )\n",
            "  Output layer has been built (in: 10 ) (out: 1 )\n",
            "Partition 0 early stopping after 134 epochs\n",
            "Lags 6/38 partition 0 cell UntiedLSTM 0.8989583333333333 3.7644799379304614 46.61725867955295\n",
            "Units 10\n",
            "Building net...\n",
            "  Untied cell has been built (in: 16 ) (out: 10 )\n",
            "  Output layer has been built (in: 10 ) (out: 1 )\n",
            "Partition 0 early stopping after 30 epochs\n",
            "Lags 10/38 partition 0 cell UntiedLSTM 0.8993055555555556 3.965182375817352 86.59568915381\n",
            "Units 10\n",
            "Building net...\n",
            "  Untied cell has been built (in: 16 ) (out: 10 )\n",
            "  Output layer has been built (in: 10 ) (out: 1 )\n",
            "Partition 0 early stopping after 108 epochs\n",
            "Lags 14/38 partition 0 cell UntiedLSTM 0.8905381944444445 3.7650369278525524 51.67271442745997\n",
            "Units 10\n",
            "Building net...\n",
            "  Untied cell has been built (in: 16 ) (out: 10 )\n",
            "  Output layer has been built (in: 10 ) (out: 1 )\n",
            "Partition 0 early stopping after 103 epochs\n",
            "Lags 18/38 partition 0 cell UntiedLSTM 0.8885416666666667 3.78525341929926 53.01611378637693\n",
            "Units 10\n",
            "Building net...\n",
            "  Untied cell has been built (in: 16 ) (out: 10 )\n",
            "  Output layer has been built (in: 10 ) (out: 1 )\n",
            "Partition 0 early stopping after 31 epochs\n",
            "Lags 22/38 partition 0 cell UntiedLSTM 0.8954861111111112 3.974539316638967 91.7410860697985\n",
            "Units 10\n",
            "Building net...\n",
            "  Untied cell has been built (in: 16 ) (out: 10 )\n",
            "  Output layer has been built (in: 10 ) (out: 1 )\n",
            "Partition 0 early stopping after 33 epochs\n",
            "Lags 26/38 partition 0 cell UntiedLSTM 0.8975694444444444 4.003808327968517 89.93870063781908\n",
            "Units 10\n",
            "Building net...\n",
            "  Untied cell has been built (in: 16 ) (out: 10 )\n",
            "  Output layer has been built (in: 10 ) (out: 1 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXAOV1W6v4PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "pd.DataFrame(mse[:,:,0]).to_csv('mse_lstm.csv')\n",
        "!cp mse_un.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7zgJ1getem9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(time[:,:,0]).to_csv('time_lstm.csv')\n",
        "!cp time_un.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHRvIqeMaVF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(icp[:,:,0]).to_csv('icp_lstm.csv')\n",
        "!cp icp_un.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UdnUj0TaZjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(mil[:,:,0]).to_csv('mil_lstm.csv')\n",
        "!cp mil_un.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiuwFj0mFnAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot([time.seconds for time in time_lst], 'bo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvkJbNPGxgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot([abs(icp - 0.90) for icp in icp_lst], 'bo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz0LyeufUTsH",
        "colab_type": "text"
      },
      "source": [
        "## Basic MSE: 18.59 - 19.24\n",
        "## Tied MSE: 11.41 - 12.06\n",
        "## Untied MSE: 11.08 - 11.71\n",
        "\n",
        "## 16 Units tied 32.82-33.80 (41 epochs - 15 mins), icp 0.899. LR 1e-2, decay after 50. Batch size 40.\n",
        " - Add scale prior (mse improved 31.15-31.69) 65 epochs - 26 mins icp 0.883\n",
        " - Increase batch size \n",
        "   - 150 -> MSE 37.18, 81 epochs - 8 minutes icp 0.880 (miss high points more)\n",
        "   - 100 -> MSE 35.17, 59 epochs - 9 minutes icp 0.887 (still miss high but better)\n",
        "   - 90 -> MSE 34.32, 68 epochs - 12 minutes icp ___ (still miss high but better than 100)\n",
        "   - 80 -> MSE 32.39, 54 epochs - 11 minutes icp 0.899 (seems to miss as many highs as 90 but best icp)\n",
        " - Decrease LR Decay (last model was 54 epochs with decay at 50)\n",
        "   - 30 -> Stopped it early as > 90 epoch and 36+ mse\n",
        "   - 45 -> MSE got worse\n",
        " - Different num units (last MSE 32.39, 11 mins)\n",
        "   - 50 -> 31.31 (66 epochs, 13 mins) icp 0.887\n",
        "   - 64 -> 32.47 (89 epochs, 18 mins) icp 0.887\n",
        "    - With decay off -> 34.34 (84 epochs, 17 mins) icp 0.889\n",
        " - Back to mixture prior and increase dropout prob\n",
        "    - 64 units -> prob = .20, mse 39.15 icp 0.881\n",
        "    - 64 units -> prob = .10, mse 35.75 icp 0.865 \n",
        " - Add mixture to dense layer\n",
        "    - 32 units -> prob = .10, mse 37.38 icp 0.893\n",
        "    - 32 units -> prob = .05, mse \n",
        " - Have a last try at KL reweight\n",
        " - Train a medium model for different lags and study performance and time taken\n",
        " - Consider log(1+x) transformation before standardisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8RctkAj3OIo",
        "colab_type": "text"
      },
      "source": [
        "<a id='testing'></a>\n",
        "# Model Testing\n",
        "[Click to get back to basic rnn cell](#basic_model)\n",
        "\n",
        "[Click to get back to untied cell](#untied_model)\n",
        "\n",
        "[Click to get back to tied cell](#tied_model)\n",
        "\n",
        "[Click to get back to rnn](#rnn)\n",
        "\n",
        "[Click to get back to model training](#training)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4RU7lG83OIp",
        "colab_type": "text"
      },
      "source": [
        "## Assessing Link / Route Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k0yPFBX3OIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_route(net, nsamples, data_test):\n",
        "  for net in nets:\n",
        "    nsamples = 100\n",
        "    y_pred = np.empty((nsamples, y_test.shape[0],y_test.shape[2]))\n",
        "    T = 0\n",
        "    for x_batch, _ in data_test:\n",
        "        x = tf.cast(x_batch,tf.float32)\n",
        "        for n in range(nsamples):\n",
        "          out = net(x, training=False, sampling=True)\n",
        "          y_pred[n, T*batch_size:(T+1)*batch_size] = tfd.MultivariateNormalTriL(out, scale_tril=net.y_std).sample()\n",
        "        T = T+1\n",
        "    y_pred_all = np.sum(y_pred*y_std_test[:,0] + y_mean_test[:,0], axis=2)\n",
        "    y_test_all = np.sum(y_test[:,0]*y_std_test[:,0] + y_mean_test[:,0], axis=1)\n",
        "\n",
        "    quantiles = np.array([0.05, 0.95])\n",
        "    y_mean = np.mean(y_pred_all, axis=0)\n",
        "    q1 = np.quantile(y_pred_all, quantiles[ 0], axis=0)\n",
        "    q2 = np.quantile(y_pred_all, quantiles[ 1], axis=0)\n",
        "\n",
        "    print(\"ICP: %.3f\" % (1 - (np.sum(y_test_all < q1) + np.sum(y_test_all > q2) )/len(y_test_all)))\n",
        "    plt.figure(figsize=(20,6)) \n",
        "    plt.plot(y_ix_test, q1, 'r', alpha=0.3)\n",
        "    plt.plot(y_ix_test, q2, 'r', alpha=0.3)\n",
        "    plt.plot(y_ix_test, y_mean, 'g', alpha=1)\n",
        "    plt.plot(y_ix_test, y_test_all, 'bo', alpha = 0.5) \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDDamAAIbckZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_route(net, 10, data_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16U8-Gtv3OIt",
        "colab_type": "text"
      },
      "source": [
        "## Assessing Route Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g_AeFEq3OI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"ICP: %.3f\" % (1 - (np.sum(y_test_all < q1) + np.sum(y_test_all > q2) )/len(y_test_all)))\n",
        "plt.figure(figsize=(20,6)) \n",
        "plt.plot(y_ix_test, q1, 'r', alpha=0.3)\n",
        "plt.plot(y_ix_test, q2, 'r', alpha=0.3)\n",
        "plt.plot(y_ix_test, y_mean, 'g', alpha=1)\n",
        "plt.plot(y_ix_test, y_test_all, 'bo', alpha = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-M5wIYdIpTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}