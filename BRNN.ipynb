{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your computer has the right versions installed, this BRNN implementation will ONLY work in TF2.0, you might need to get Gast as well if you get an error related to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Xszd5fWa4HK_",
    "outputId": "46e6fad4-0ea2-418e-a0f8-615d04b752e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BayesianNN-DQR'...\n",
      "remote: Enumerating objects: 101, done.\u001b[K\n",
      "remote: Counting objects:   0% (1/101)\u001b[K\r",
      "remote: Counting objects:   1% (2/101)\u001b[K\r",
      "remote: Counting objects:   2% (3/101)\u001b[K\r",
      "remote: Counting objects:   3% (4/101)\u001b[K\r",
      "remote: Counting objects:   4% (5/101)\u001b[K\r",
      "remote: Counting objects:   5% (6/101)\u001b[K\r",
      "remote: Counting objects:   6% (7/101)\u001b[K\r",
      "remote: Counting objects:   7% (8/101)\u001b[K\r",
      "remote: Counting objects:   8% (9/101)\u001b[K\r",
      "remote: Counting objects:   9% (10/101)\u001b[K\r",
      "remote: Counting objects:  10% (11/101)\u001b[K\r",
      "remote: Counting objects:  11% (12/101)\u001b[K\r",
      "remote: Counting objects:  12% (13/101)\u001b[K\r",
      "remote: Counting objects:  13% (14/101)\u001b[K\r",
      "remote: Counting objects:  14% (15/101)\u001b[K\r",
      "remote: Counting objects:  15% (16/101)\u001b[K\r",
      "remote: Counting objects:  16% (17/101)\u001b[K\r",
      "remote: Counting objects:  17% (18/101)\u001b[K\r",
      "remote: Counting objects:  18% (19/101)\u001b[K\r",
      "remote: Counting objects:  19% (20/101)\u001b[K\r",
      "remote: Counting objects:  20% (21/101)\u001b[K\r",
      "remote: Counting objects:  21% (22/101)\u001b[K\r",
      "remote: Counting objects:  22% (23/101)\u001b[K\r",
      "remote: Counting objects:  23% (24/101)\u001b[K\r",
      "remote: Counting objects:  24% (25/101)\u001b[K\r",
      "remote: Counting objects:  25% (26/101)\u001b[K\r",
      "remote: Counting objects:  26% (27/101)\u001b[K\r",
      "remote: Counting objects:  27% (28/101)\u001b[K\r",
      "remote: Counting objects:  28% (29/101)\u001b[K\r",
      "remote: Counting objects:  29% (30/101)\u001b[K\r",
      "remote: Counting objects:  30% (31/101)\u001b[K\r",
      "remote: Counting objects:  31% (32/101)\u001b[K\r",
      "remote: Counting objects:  32% (33/101)\u001b[K\r",
      "remote: Counting objects:  33% (34/101)\u001b[K\r",
      "remote: Counting objects:  34% (35/101)\u001b[K\r",
      "remote: Counting objects:  35% (36/101)\u001b[K\r",
      "remote: Counting objects:  36% (37/101)\u001b[K\r",
      "remote: Counting objects:  37% (38/101)\u001b[K\r",
      "remote: Counting objects:  38% (39/101)\u001b[K\r",
      "remote: Counting objects:  39% (40/101)\u001b[K\r",
      "remote: Counting objects:  40% (41/101)\u001b[K\r",
      "remote: Counting objects:  41% (42/101)\u001b[K\r",
      "remote: Counting objects:  42% (43/101)\u001b[K\r",
      "remote: Counting objects:  43% (44/101)\u001b[K\r",
      "remote: Counting objects:  44% (45/101)\u001b[K\r",
      "remote: Counting objects:  45% (46/101)\u001b[K\r",
      "remote: Counting objects:  46% (47/101)\u001b[K\r",
      "remote: Counting objects:  47% (48/101)\u001b[K\r",
      "remote: Counting objects:  48% (49/101)\u001b[K\r",
      "remote: Counting objects:  49% (50/101)\u001b[K\r",
      "remote: Counting objects:  50% (51/101)\u001b[K\r",
      "remote: Counting objects:  51% (52/101)\u001b[K\r",
      "remote: Counting objects:  52% (53/101)\u001b[K\r",
      "remote: Counting objects:  53% (54/101)\u001b[K\r",
      "remote: Counting objects:  54% (55/101)\u001b[K\r",
      "remote: Counting objects:  55% (56/101)\u001b[K\r",
      "remote: Counting objects:  56% (57/101)\u001b[K\r",
      "remote: Counting objects:  57% (58/101)\u001b[K\r",
      "remote: Counting objects:  58% (59/101)\u001b[K\r",
      "remote: Counting objects:  59% (60/101)\u001b[K\r",
      "remote: Counting objects:  60% (61/101)\u001b[K\r",
      "remote: Counting objects:  61% (62/101)\u001b[K\r",
      "remote: Counting objects:  62% (63/101)\u001b[K\r",
      "remote: Counting objects:  63% (64/101)\u001b[K\r",
      "remote: Counting objects:  64% (65/101)\u001b[K\r",
      "remote: Counting objects:  65% (66/101)\u001b[K\r",
      "remote: Counting objects:  66% (67/101)\u001b[K\r",
      "remote: Counting objects:  67% (68/101)\u001b[K\r",
      "remote: Counting objects:  68% (69/101)\u001b[K\r",
      "remote: Counting objects:  69% (70/101)\u001b[K\r",
      "remote: Counting objects:  70% (71/101)\u001b[K\r",
      "remote: Counting objects:  71% (72/101)\u001b[K\r",
      "remote: Counting objects:  72% (73/101)\u001b[K\r",
      "remote: Counting objects:  73% (74/101)\u001b[K\r",
      "remote: Counting objects:  74% (75/101)\u001b[K\r",
      "remote: Counting objects:  75% (76/101)\u001b[K\r",
      "remote: Counting objects:  76% (77/101)\u001b[K\r",
      "remote: Counting objects:  77% (78/101)\u001b[K\r",
      "remote: Counting objects:  78% (79/101)\u001b[K\r",
      "remote: Counting objects:  79% (80/101)\u001b[K\r",
      "remote: Counting objects:  80% (81/101)\u001b[K\r",
      "remote: Counting objects:  81% (82/101)\u001b[K\r",
      "remote: Counting objects:  82% (83/101)\u001b[K\r",
      "remote: Counting objects:  83% (84/101)\u001b[K\r",
      "remote: Counting objects:  84% (85/101)\u001b[K\r",
      "remote: Counting objects:  85% (86/101)\u001b[K\r",
      "remote: Counting objects:  86% (87/101)\u001b[K\r",
      "remote: Counting objects:  87% (88/101)\u001b[K\r",
      "remote: Counting objects:  88% (89/101)\u001b[K\r",
      "remote: Counting objects:  89% (90/101)\u001b[K\r",
      "remote: Counting objects:  90% (91/101)\u001b[K\r",
      "remote: Counting objects:  91% (92/101)\u001b[K\r",
      "remote: Counting objects:  92% (93/101)\u001b[K\r",
      "remote: Counting objects:  93% (94/101)\u001b[K\r",
      "remote: Counting objects:  94% (95/101)\u001b[K\r",
      "remote: Counting objects:  95% (96/101)\u001b[K\r",
      "remote: Counting objects:  96% (97/101)\u001b[K\r",
      "remote: Counting objects:  97% (98/101)\u001b[K\r",
      "remote: Counting objects:  98% (99/101)\u001b[K\r",
      "remote: Counting objects:  99% (100/101)\u001b[K\r",
      "remote: Counting objects: 100% (101/101)\u001b[K\r",
      "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
      "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
      "remote: Total 1738 (delta 68), reused 0 (delta 0), pack-reused 1637\u001b[K\n",
      "Receiving objects: 100% (1738/1738), 1.29 GiB | 14.95 MiB/s, done.\n",
      "Resolving deltas: 100% (481/481), done.\n",
      "Checking out files: 100% (1142/1142), done.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__ != '2.0.0-rc1':\n",
    "  !pip install tensorflow-gpu==2.0.0-rc1\n",
    "  !pip install tensorflow_probability==0.8.0-rc0\n",
    "  !pip install numpy==1.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "oi8RJXwPfA9L",
    "outputId": "d19ac690-0615-4c0c-d2b0-c0c15c358517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Probability Version .0.8.0-rc0\n",
      "Tensorflow Version .2.0.0-rc1\n",
      "Numpy Version .1.17.2\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error as eval_mse\n",
    "from sklearn.metrics import median_absolute_error as eval_mae\n",
    "from datetime import datetime\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "print(\"Tensorflow Probability Version .{}\".format(tfp.__version__))\n",
    "print(\"Tensorflow Version .{}\".format(tf.__version__))\n",
    "print(\"Numpy Version .{}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following modules can't be loaded just take the functions from Github and paste them into this Notebook :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BNN.prior import MixturePrior\n",
    "from BNN.VariationalPosterior import VariationalPosterior\n",
    "from BNN.cells.BayesianBasicCell import MinimalRNNCell\n",
    "from BNN.cells.BayesianUntiedLSTM import BayesianLSTMCell_Untied\n",
    "from BNN.cells.BayesianTiedLSTM import BayesianLSTMCellTied\n",
    "from load import sort_and_order, skip_row, write_3d, skip_row, transform, fit_scale, roll, sort_links, tod_interval, split_df_with_val, tilted_loss_np_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjt3tx_O4fGL"
   },
   "outputs": [],
   "source": [
    "class BayesianRNN(tf.keras.Model):\n",
    "    def __init__(self, num_units, num_links, batch_size, init, cell_type, prior, **kwargs):\n",
    "        super(BayesianRNN, self).__init__(**kwargs)\n",
    "        self.cell_type = cell_type\n",
    "        self.init = init\n",
    "        self.num_units_lst = num_units\n",
    "        self.num_links = num_links\n",
    "        self.batch_size = batch_size\n",
    "        self.cell_prior = prior\n",
    "        self.prior = prior\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        print(\"Building net...\")\n",
    "        self.cell_lst = []\n",
    "        state_size = self.num_links\n",
    "        for i, num_units in enumerate(self.num_units_lst):\n",
    "          if self.cell_type == 'Basic':\n",
    "              self.cell_lst.append(MinimalRNNCell(num_units, training=True, init=self.init, prior=self.cell_prior))\n",
    "          elif self.cell_type == 'TiedLSTM':\n",
    "              self.cell_lst.append(BayesianLSTMCellTied(num_units, training=True, init=self.init, prior=self.cell_prior))\n",
    "          else:\n",
    "              self.cell_lst.append(BayesianLSTMCell_Untied(num_units, training=True, init=self.init, prior=self.cell_prior))\n",
    "          self.cell_lst[-1].initialise_cell(state_size)\n",
    "          state_size = num_units\n",
    "            \n",
    "        self.weight_mu = self.add_weight(shape=(self.num_units_lst[-1],self.num_links),\n",
    "                                 initializer=self.init,\n",
    "                                 name='weight_mu')\n",
    "        self.weight_rho = self.add_weight(shape=(self.num_units_lst[-1],self.num_links),\n",
    "                                 initializer=self.init,\n",
    "                                 name='weight_mu')\n",
    "        self.bias_mu = self.add_weight(shape=(self.num_links,),\n",
    "                                     initializer=self.init,\n",
    "                                     name='bias_mu', trainable=True)\n",
    "        self.bias_rho = self.add_weight(shape=(self.num_links,),\n",
    "                                     initializer=self.init,\n",
    "                                     name='bias_mu', trainable=True)\n",
    "        self.weight_dist = VariationalPosterior(self.weight_mu, self.weight_rho) \n",
    "        self.bias_dist = VariationalPosterior(self.bias_mu, self.bias_rho)     \n",
    "        print(\"  Output layer has been built (in:\", self.num_units_lst[-1], \") (out:\", 1, \")\")\n",
    "\n",
    "        ## The diagonal of the correlation matrix\n",
    "        self.scale_prior = tfd.LKJ(dimension=self.num_links, concentration=10, input_output_cholesky=True)\n",
    "        self.y_rho = self.add_weight(shape=(self.num_links*((self.num_links-1)/2 + 1),), \n",
    "                                     initializer='zeros',\n",
    "                                     name='y_rho',\n",
    "                                     trainable=True)\n",
    "        self.built = True\n",
    "    @property\n",
    "    def y_std(self):\n",
    "        cor = tfb.ScaleTriL(diag_bijector=tfb.Softplus(),\n",
    "                            diag_shift=None)\n",
    "        return cor.forward(self.y_rho)\n",
    "\n",
    "    def call(self, batch_x, training, sampling):\n",
    "        self.weight = self.weight_dist.sample(training, sampling)\n",
    "        self.bias = self.bias_dist.sample(training, sampling)\n",
    "        if training:\n",
    "            self.log_prior_dense = sum_all(self.prior.log_prob(self.weight)) + sum_all(self.prior.log_prob(self.bias))\n",
    "            self.log_variational_posterior_dense  = self.weight_dist.log_prob(self.weight) \n",
    "            self.log_variational_posterior_dense += self.bias_dist.log_prob(self.bias)\n",
    "        for cell in self.cell_lst:\n",
    "          cell.is_training = training\n",
    "          cell.sampling = sampling\n",
    "\n",
    "        inputs = tf.convert_to_tensor(batch_x)\n",
    "        rnn = tf.keras.layers.RNN(self.cell_lst)\n",
    "        ## RNN layer\n",
    "        final_rnn_output = rnn(inputs)\n",
    "        ## Dense layer\n",
    "        self.outputs = tf.linalg.matmul(final_rnn_output, self.weight) + self.bias   \n",
    "        return self.outputs\n",
    "    \n",
    "    def log_prior(self):\n",
    "        return sum(sum_all(cell.log_prior) for cell in self.cell_lst) + sum_all(self.log_prior_dense) + sum_all(self.scale_prior.log_prob(self.y_std))\n",
    "    \n",
    "    def log_variational_posterior(self):\n",
    "        return sum(sum_all(cell.log_variational_posterior) for cell in self.cell_lst) + sum_all(self.log_variational_posterior_dense)\n",
    "    \n",
    "    def elbo(self, batch_x, batch_y, batch_ind, num_batches,  training, sampling=True):\n",
    "        output = self(batch_x, training, sampling)\n",
    "        assert(batch_y.shape[1] == self.num_links)\n",
    "        assert(output.shape == batch_y.shape)\n",
    "        pred_dist = tfd.MultivariateNormalTriL(output, scale_tril=self.y_std)\n",
    "        self.nll = -tf.math.reduce_sum(pred_dist.log_prob(batch_y))\n",
    "        kl_weight = 2**(num_batches - batch_ind) / (2**num_batches - 1)\n",
    "        return (self.log_variational_posterior() - self.log_prior())/num_batches + self.nll, sum_all((output - batch_y)**2) / self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Across Test Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpGG4j0B4Twv"
   },
   "outputs": [],
   "source": [
    "num_units_basic_l10 = 6\n",
    "num_units_basic_l20 = 10\n",
    "num_units_tiedLSTM_l10 = 54\n",
    "num_units_tiedLSTM_l20 = 66\n",
    "num_units_untiedLSTM_l10 = 20\n",
    "num_units_untiedLSTM_l20 = 30\n",
    "\n",
    "param_lst = [num_units_basic_l10,\n",
    "            num_units_basic_l20,\n",
    "            num_units_tiedLSTM_l10,\n",
    "            num_units_tiedLSTM_l20,\n",
    "            num_units_untiedLSTM_l10,\n",
    "            num_units_untiedLSTM_l20,]\n",
    "mtypes = ['Basic', 'Basic', 'TiedLSTM', 'TiedLSTM', 'UntiedLSTM', 'UntiedLSTM'] \n",
    "lag_lst = [10,20,10,20,10,20] \n",
    "prior = MixturePrior(0.10, 1, np.exp(-6))\n",
    "\n",
    "quantiles = np.array([0.005, 0.995, 0.025, 0.975, 0.05, 0.95, 0.10, 0.90, 0.20, 0.80, 0.30, 0.70, 0.40, 0.60])\n",
    "pred_ints = np.array([0.99,         0.95,         0.90,       0.80,       0.60,       0.40,       0.20])\n",
    "\n",
    "start = datetime.strptime('19/01/21', \"%y/%m/%d\")\n",
    "end   = datetime.strptime('19/04/14', \"%y/%m/%d\")\n",
    "period = (end - start).days\n",
    "period_train_days = 7*4  ## Train on 4 weeks\n",
    "period_val_days = 7      ## Validate training on 1 week\n",
    "period_test_days =  7    ## Test on  1 week\n",
    "advance_days = 7         ## Advance by 1 week\n",
    "num_partitions = int((period-period_train_days-period_test_days)/advance_days)\n",
    "\n",
    "init = 'uniform'\n",
    "num_links = 16\n",
    "preds = 1\n",
    "batch_size = 80\n",
    "epochs = 100\n",
    "patience = 8\n",
    "init_lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "o2s38VcpH7Kv",
    "outputId": "d3aacdcd-6de0-4de2-d5b6-a6e8d2d8c6d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian net with Basic cell and 10 lags.Building net...\n",
      "  Basic cell has been built (in: 16 ) (out: 6 )\n",
      "  Output layer has been built (in: 6 ) (out: 1 )\n",
      "   Partition 1/6 Training on weeks [1 2 3 4] validating training on week 5 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2520: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Early stopping after 40 epochs\n",
      "      Testing on week 6\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.964, MIL (0.99) 76.845 ICP (0.95) 0.942, MIL (0.95) 64.356 ICP (0.90) 0.917, MIL (0.90) 55.466 ICP (0.80) 0.853, MIL (0.80) 43.916 ICP (0.60) 0.693, MIL (0.60) 29.155 ICP (0.40) 0.491, MIL (0.40) 18.262 ICP (0.20) 0.258, MIL (0.20) 8.855\n",
      "    Quantile errors (route)    ICP (0.99) 0.984, MIL (0.99) 433.174 ICP (0.95) 0.976, MIL (0.95) 362.900 ICP (0.90) 0.952, MIL (0.90) 311.847 ICP (0.80) 0.901, MIL (0.80) 246.204 ICP (0.60) 0.743, MIL (0.60) 163.575 ICP (0.40) 0.547, MIL (0.40) 102.571 ICP (0.20) 0.274, MIL (0.20) 49.197\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.06 (0.975): 0.11 (0.05): 0.11 (0.95): 0.17 (0.1): 0.17 (0.9): 0.25 (0.2): 0.26 (0.8): 0.36 (0.3): 0.32 (0.7): 0.41 (0.4): 0.36 (0.6): 0.42 \n",
      "    Mean errors by sampling (avg link) MSE 0.084 MAE 0.154 MAPE 0.153\n",
      "    Mean errors by sampling (route)    MSE 1.691 MAE 0.775 MAPE 0.046\n",
      "    Mean errors by mean parameters (avg link) MSE 0.083 MAE 0.153 MAPE 0.152\n",
      "    Mean errors by mean parameters (route)    MSE 1.634 MAE 0.045 MAPE 0.045\n",
      "   Partition 2/6 Training on weeks [2 3 4 5] validating training on week 6 Early stopping after 12 epochs\n",
      "      Testing on week 7\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.964, MIL (0.99) 80.652 ICP (0.95) 0.945, MIL (0.95) 67.552 ICP (0.90) 0.920, MIL (0.90) 58.154 ICP (0.80) 0.858, MIL (0.80) 46.000 ICP (0.60) 0.700, MIL (0.60) 30.542 ICP (0.40) 0.498, MIL (0.40) 19.095 ICP (0.20) 0.264, MIL (0.20) 9.207\n",
      "    Quantile errors (route)    ICP (0.99) 0.974, MIL (0.99) 470.635 ICP (0.95) 0.957, MIL (0.95) 394.028 ICP (0.90) 0.935, MIL (0.90) 340.979 ICP (0.80) 0.895, MIL (0.80) 269.685 ICP (0.60) 0.735, MIL (0.60) 178.725 ICP (0.40) 0.523, MIL (0.40) 111.848 ICP (0.20) 0.258, MIL (0.20) 54.314\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.07 (0.025): 0.06 (0.975): 0.13 (0.05): 0.11 (0.95): 0.18 (0.1): 0.18 (0.9): 0.27 (0.2): 0.28 (0.8): 0.37 (0.3): 0.34 (0.7): 0.42 (0.4): 0.38 (0.6): 0.42 \n",
      "    Mean errors by sampling (avg link) MSE 0.119 MAE 0.152 MAPE 0.156\n",
      "    Mean errors by sampling (route)    MSE 2.651 MAE 0.885 MAPE 0.053\n",
      "    Mean errors by mean parameters (avg link) MSE 0.116 MAE 0.150 MAPE 0.152\n",
      "    Mean errors by mean parameters (route)    MSE 2.584 MAE 0.053 MAPE 0.053\n",
      "   Partition 3/6 Training on weeks [3 4 5 6] validating training on week 7 Early stopping after 37 epochs\n",
      "      Testing on week 8\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.965, MIL (0.99) 72.925 ICP (0.95) 0.946, MIL (0.95) 61.095 ICP (0.90) 0.916, MIL (0.90) 52.626 ICP (0.80) 0.851, MIL (0.80) 41.625 ICP (0.60) 0.687, MIL (0.60) 27.650 ICP (0.40) 0.481, MIL (0.40) 17.276 ICP (0.20) 0.252, MIL (0.20) 8.352\n",
      "    Quantile errors (route)    ICP (0.99) 0.958, MIL (0.99) 387.539 ICP (0.95) 0.934, MIL (0.95) 325.399 ICP (0.90) 0.907, MIL (0.90) 280.533 ICP (0.80) 0.828, MIL (0.80) 221.624 ICP (0.60) 0.661, MIL (0.60) 146.635 ICP (0.40) 0.463, MIL (0.40) 91.507 ICP (0.20) 0.231, MIL (0.20) 44.724\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.06 (0.975): 0.12 (0.05): 0.10 (0.95): 0.17 (0.1): 0.17 (0.9): 0.24 (0.2): 0.26 (0.8): 0.34 (0.3): 0.32 (0.7): 0.39 (0.4): 0.37 (0.6): 0.40 \n",
      "    Mean errors by sampling (avg link) MSE 0.105 MAE 0.149 MAPE 0.152\n",
      "    Mean errors by sampling (route)    MSE 2.967 MAE 0.814 MAPE 0.052\n",
      "    Mean errors by mean parameters (avg link) MSE 0.103 MAE 0.145 MAPE 0.149\n",
      "    Mean errors by mean parameters (route)    MSE 2.874 MAE 0.051 MAPE 0.051\n",
      "   Partition 4/6 Training on weeks [4 5 6 7] validating training on week 8 Early stopping after 19 epochs\n",
      "      Testing on week 9\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.963, MIL (0.99) 67.734 ICP (0.95) 0.939, MIL (0.95) 56.794 ICP (0.90) 0.907, MIL (0.90) 48.923 ICP (0.80) 0.833, MIL (0.80) 38.630 ICP (0.60) 0.662, MIL (0.60) 25.672 ICP (0.40) 0.456, MIL (0.40) 16.034 ICP (0.20) 0.233, MIL (0.20) 7.760\n",
      "    Quantile errors (route)    ICP (0.99) 0.943, MIL (0.99) 341.493 ICP (0.95) 0.915, MIL (0.95) 285.123 ICP (0.90) 0.876, MIL (0.90) 245.810 ICP (0.80) 0.795, MIL (0.80) 194.891 ICP (0.60) 0.600, MIL (0.60) 128.867 ICP (0.40) 0.417, MIL (0.40) 80.475 ICP (0.20) 0.203, MIL (0.20) 39.317\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.05 (0.025): 0.05 (0.975): 0.10 (0.05): 0.09 (0.95): 0.15 (0.1): 0.16 (0.9): 0.22 (0.2): 0.25 (0.8): 0.31 (0.3): 0.31 (0.7): 0.36 (0.4): 0.35 (0.6): 0.38 \n",
      "    Mean errors by sampling (avg link) MSE 0.094 MAE 0.148 MAPE 0.150\n",
      "    Mean errors by sampling (route)    MSE 2.078 MAE 0.800 MAPE 0.050\n",
      "    Mean errors by mean parameters (avg link) MSE 0.092 MAE 0.146 MAPE 0.148\n",
      "    Mean errors by mean parameters (route)    MSE 2.079 MAE 0.050 MAPE 0.050\n",
      "   Partition 5/6 Training on weeks [5 6 7 8] validating training on week 9 Early stopping after 11 epochs\n",
      "      Testing on week 10\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.948, MIL (0.99) 65.519 ICP (0.95) 0.923, MIL (0.95) 54.890 ICP (0.90) 0.888, MIL (0.90) 47.346 ICP (0.80) 0.814, MIL (0.80) 37.440 ICP (0.60) 0.636, MIL (0.60) 24.834 ICP (0.40) 0.431, MIL (0.40) 15.530 ICP (0.20) 0.212, MIL (0.20) 7.488\n",
      "    Quantile errors (route)    ICP (0.99) 0.929, MIL (0.99) 333.474 ICP (0.95) 0.883, MIL (0.95) 278.142 ICP (0.90) 0.843, MIL (0.90) 239.624 ICP (0.80) 0.757, MIL (0.80) 189.081 ICP (0.60) 0.580, MIL (0.60) 125.965 ICP (0.40) 0.400, MIL (0.40) 79.202 ICP (0.20) 0.188, MIL (0.20) 38.217\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.18 (0.025): 0.06 (0.975): 0.23 (0.05): 0.11 (0.95): 0.28 (0.1): 0.18 (0.9): 0.35 (0.2): 0.29 (0.8): 0.43 (0.3): 0.37 (0.7): 0.48 (0.4): 0.43 (0.6): 0.49 \n",
      "    Mean errors by sampling (avg link) MSE 0.228 MAE 0.150 MAPE 0.157\n",
      "    Mean errors by sampling (route)    MSE 11.873 MAE 0.834 MAPE 0.059\n",
      "    Mean errors by mean parameters (avg link) MSE 0.227 MAE 0.148 MAPE 0.156\n",
      "    Mean errors by mean parameters (route)    MSE 11.737 MAE 0.059 MAPE 0.059\n",
      "   Partition 6/6 Training on weeks [6 7 8 9] validating training on week 10 Early stopping after 10 epochs\n",
      "      Testing on week 11\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.959, MIL (0.99) 68.556 ICP (0.95) 0.933, MIL (0.95) 57.521 ICP (0.90) 0.898, MIL (0.90) 49.568 ICP (0.80) 0.819, MIL (0.80) 39.210 ICP (0.60) 0.640, MIL (0.60) 26.019 ICP (0.40) 0.438, MIL (0.40) 16.296 ICP (0.20) 0.220, MIL (0.20) 7.866\n",
      "    Quantile errors (route)    ICP (0.99) 0.921, MIL (0.99) 344.130 ICP (0.95) 0.876, MIL (0.95) 288.721 ICP (0.90) 0.838, MIL (0.90) 249.879 ICP (0.80) 0.748, MIL (0.80) 196.983 ICP (0.60) 0.590, MIL (0.60) 130.964 ICP (0.40) 0.384, MIL (0.40) 81.809 ICP (0.20) 0.199, MIL (0.20) 39.626\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.06 (0.025): 0.06 (0.975): 0.11 (0.05): 0.10 (0.95): 0.16 (0.1): 0.16 (0.9): 0.23 (0.2): 0.26 (0.8): 0.33 (0.3): 0.32 (0.7): 0.38 (0.4): 0.37 (0.6): 0.40 \n",
      "    Mean errors by sampling (avg link) MSE 0.098 MAE 0.158 MAPE 0.163\n",
      "    Mean errors by sampling (route)    MSE 2.362 MAE 0.851 MAPE 0.057\n",
      "    Mean errors by mean parameters (avg link) MSE 0.097 MAE 0.156 MAPE 0.160\n",
      "    Mean errors by mean parameters (route)    MSE 2.336 MAE 0.056 MAPE 0.056\n",
      "Times for model (seconds)\n",
      "[756. 239. 687. 370. 224. 206.]\n",
      "\n",
      "Bayesian net with Basic cell and 20 lags.Building net...\n",
      "  Basic cell has been built (in: 16 ) (out: 10 )\n",
      "  Output layer has been built (in: 10 ) (out: 1 )\n",
      "   Partition 1/6 Training on weeks [1 2 3 4] validating training on week 5 Early stopping after 18 epochs\n",
      "      Testing on week 6\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.970, MIL (0.99) 83.311 ICP (0.95) 0.955, MIL (0.95) 69.822 ICP (0.90) 0.935, MIL (0.90) 60.161 ICP (0.80) 0.878, MIL (0.80) 47.598 ICP (0.60) 0.722, MIL (0.60) 31.620 ICP (0.40) 0.519, MIL (0.40) 19.793 ICP (0.20) 0.272, MIL (0.20) 9.595\n",
      "    Quantile errors (route)    ICP (0.99) 0.984, MIL (0.99) 482.656 ICP (0.95) 0.970, MIL (0.95) 405.004 ICP (0.90) 0.957, MIL (0.90) 349.298 ICP (0.80) 0.914, MIL (0.80) 275.759 ICP (0.60) 0.770, MIL (0.60) 182.775 ICP (0.40) 0.555, MIL (0.40) 113.698 ICP (0.20) 0.315, MIL (0.20) 55.424\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.05 (0.025): 0.06 (0.975): 0.11 (0.05): 0.11 (0.95): 0.17 (0.1): 0.18 (0.9): 0.26 (0.2): 0.27 (0.8): 0.37 (0.3): 0.32 (0.7): 0.41 (0.4): 0.36 (0.6): 0.42 \n",
      "    Mean errors by sampling (avg link) MSE 0.087 MAE 0.155 MAPE 0.155\n",
      "    Mean errors by sampling (route)    MSE 1.906 MAE 0.774 MAPE 0.048\n",
      "    Mean errors by mean parameters (avg link) MSE 0.084 MAE 0.154 MAPE 0.154\n",
      "    Mean errors by mean parameters (route)    MSE 1.793 MAE 0.047 MAPE 0.047\n",
      "   Partition 2/6 Training on weeks [2 3 4 5] validating training on week 6 Early stopping after 27 epochs\n",
      "      Testing on week 7\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.966, MIL (0.99) 81.830 ICP (0.95) 0.947, MIL (0.95) 68.498 ICP (0.90) 0.924, MIL (0.90) 59.033 ICP (0.80) 0.861, MIL (0.80) 46.728 ICP (0.60) 0.700, MIL (0.60) 30.996 ICP (0.40) 0.503, MIL (0.40) 19.352 ICP (0.20) 0.268, MIL (0.20) 9.337\n",
      "    Quantile errors (route)    ICP (0.99) 0.977, MIL (0.99) 473.538 ICP (0.95) 0.964, MIL (0.95) 397.551 ICP (0.90) 0.942, MIL (0.90) 342.695 ICP (0.80) 0.883, MIL (0.80) 271.438 ICP (0.60) 0.707, MIL (0.60) 180.922 ICP (0.40) 0.509, MIL (0.40) 113.198 ICP (0.20) 0.272, MIL (0.20) 54.681\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.07 (0.025): 0.07 (0.975): 0.13 (0.05): 0.11 (0.95): 0.18 (0.1): 0.19 (0.9): 0.27 (0.2): 0.28 (0.8): 0.37 (0.3): 0.34 (0.7): 0.42 (0.4): 0.38 (0.6): 0.42 \n",
      "    Mean errors by sampling (avg link) MSE 0.117 MAE 0.152 MAPE 0.156\n",
      "    Mean errors by sampling (route)    MSE 2.622 MAE 0.906 MAPE 0.054\n",
      "    Mean errors by mean parameters (avg link) MSE 0.116 MAE 0.152 MAPE 0.155\n",
      "    Mean errors by mean parameters (route)    MSE 2.619 MAE 0.053 MAPE 0.053\n",
      "   Partition 3/6 Training on weeks [3 4 5 6] validating training on week 7 Early stopping after 42 epochs\n",
      "      Testing on week 8\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.964, MIL (0.99) 71.315 ICP (0.95) 0.943, MIL (0.95) 59.770 ICP (0.90) 0.915, MIL (0.90) 51.504 ICP (0.80) 0.849, MIL (0.80) 40.733 ICP (0.60) 0.677, MIL (0.60) 27.063 ICP (0.40) 0.471, MIL (0.40) 16.939 ICP (0.20) 0.245, MIL (0.20) 8.217\n",
      "    Quantile errors (route)    ICP (0.99) 0.966, MIL (0.99) 381.239 ICP (0.95) 0.945, MIL (0.95) 317.896 ICP (0.90) 0.903, MIL (0.90) 273.263 ICP (0.80) 0.822, MIL (0.80) 216.260 ICP (0.60) 0.644, MIL (0.60) 143.938 ICP (0.40) 0.458, MIL (0.40) 90.172 ICP (0.20) 0.241, MIL (0.20) 43.370\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.06 (0.025): 0.06 (0.975): 0.11 (0.05): 0.10 (0.95): 0.16 (0.1): 0.16 (0.9): 0.24 (0.2): 0.26 (0.8): 0.33 (0.3): 0.32 (0.7): 0.38 (0.4): 0.37 (0.6): 0.40 \n",
      "    Mean errors by sampling (avg link) MSE 0.103 MAE 0.150 MAPE 0.153\n",
      "    Mean errors by sampling (route)    MSE 3.076 MAE 0.819 MAPE 0.052\n",
      "    Mean errors by mean parameters (avg link) MSE 0.103 MAE 0.148 MAPE 0.150\n",
      "    Mean errors by mean parameters (route)    MSE 3.127 MAE 0.052 MAPE 0.052\n",
      "   Partition 4/6 Training on weeks [4 5 6 7] validating training on week 8 Early stopping after 18 epochs\n",
      "      Testing on week 9\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.963, MIL (0.99) 67.121 ICP (0.95) 0.939, MIL (0.95) 56.180 ICP (0.90) 0.909, MIL (0.90) 48.412 ICP (0.80) 0.838, MIL (0.80) 38.354 ICP (0.60) 0.658, MIL (0.60) 25.477 ICP (0.40) 0.457, MIL (0.40) 15.950 ICP (0.20) 0.237, MIL (0.20) 7.709\n",
      "    Quantile errors (route)    ICP (0.99) 0.948, MIL (0.99) 342.990 ICP (0.95) 0.912, MIL (0.95) 286.835 ICP (0.90) 0.882, MIL (0.90) 246.746 ICP (0.80) 0.805, MIL (0.80) 195.107 ICP (0.60) 0.615, MIL (0.60) 129.448 ICP (0.40) 0.392, MIL (0.40) 80.863 ICP (0.20) 0.193, MIL (0.20) 39.219\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.05 (0.025): 0.05 (0.975): 0.10 (0.05): 0.09 (0.95): 0.15 (0.1): 0.16 (0.9): 0.22 (0.2): 0.24 (0.8): 0.31 (0.3): 0.31 (0.7): 0.36 (0.4): 0.35 (0.6): 0.38 \n",
      "    Mean errors by sampling (avg link) MSE 0.094 MAE 0.148 MAPE 0.149\n",
      "    Mean errors by sampling (route)    MSE 2.087 MAE 0.813 MAPE 0.050\n",
      "    Mean errors by mean parameters (avg link) MSE 0.092 MAE 0.147 MAPE 0.146\n",
      "    Mean errors by mean parameters (route)    MSE 2.080 MAE 0.049 MAPE 0.049\n",
      "   Partition 5/6 Training on weeks [5 6 7 8] validating training on week 9 Early stopping after 15 epochs\n",
      "      Testing on week 10\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.951, MIL (0.99) 65.972 ICP (0.95) 0.925, MIL (0.95) 55.232 ICP (0.90) 0.890, MIL (0.90) 47.603 ICP (0.80) 0.815, MIL (0.80) 37.707 ICP (0.60) 0.633, MIL (0.60) 25.045 ICP (0.40) 0.425, MIL (0.40) 15.667 ICP (0.20) 0.216, MIL (0.20) 7.597\n",
      "    Quantile errors (route)    ICP (0.99) 0.921, MIL (0.99) 335.300 ICP (0.95) 0.881, MIL (0.95) 280.724 ICP (0.90) 0.840, MIL (0.90) 242.620 ICP (0.80) 0.760, MIL (0.80) 192.447 ICP (0.60) 0.588, MIL (0.60) 128.428 ICP (0.40) 0.393, MIL (0.40) 80.838 ICP (0.20) 0.198, MIL (0.20) 39.172\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.18 (0.025): 0.06 (0.975): 0.23 (0.05): 0.10 (0.95): 0.28 (0.1): 0.18 (0.9): 0.35 (0.2): 0.28 (0.8): 0.43 (0.3): 0.36 (0.7): 0.48 (0.4): 0.43 (0.6): 0.48 \n",
      "    Mean errors by sampling (avg link) MSE 0.226 MAE 0.151 MAPE 0.157\n",
      "    Mean errors by sampling (route)    MSE 11.127 MAE 0.847 MAPE 0.058\n",
      "    Mean errors by mean parameters (avg link) MSE 0.222 MAE 0.149 MAPE 0.155\n",
      "    Mean errors by mean parameters (route)    MSE 10.479 MAE 0.058 MAPE 0.058\n",
      "   Partition 6/6 Training on weeks [6 7 8 9] validating training on week 10 Early stopping after 27 epochs\n",
      "      Testing on week 11\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.957, MIL (0.99) 67.868 ICP (0.95) 0.928, MIL (0.95) 56.857 ICP (0.90) 0.893, MIL (0.90) 48.987 ICP (0.80) 0.816, MIL (0.80) 38.728 ICP (0.60) 0.635, MIL (0.60) 25.675 ICP (0.40) 0.440, MIL (0.40) 16.052 ICP (0.20) 0.222, MIL (0.20) 7.738\n",
      "    Quantile errors (route)    ICP (0.99) 0.927, MIL (0.99) 343.249 ICP (0.95) 0.884, MIL (0.95) 286.725 ICP (0.90) 0.832, MIL (0.90) 246.718 ICP (0.80) 0.746, MIL (0.80) 195.623 ICP (0.60) 0.566, MIL (0.60) 130.981 ICP (0.40) 0.391, MIL (0.40) 81.458 ICP (0.20) 0.209, MIL (0.20) 39.045\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.06 (0.025): 0.06 (0.975): 0.11 (0.05): 0.10 (0.95): 0.16 (0.1): 0.16 (0.9): 0.23 (0.2): 0.26 (0.8): 0.33 (0.3): 0.32 (0.7): 0.38 (0.4): 0.37 (0.6): 0.40 \n",
      "    Mean errors by sampling (avg link) MSE 0.096 MAE 0.156 MAPE 0.160\n",
      "    Mean errors by sampling (route)    MSE 2.375 MAE 0.886 MAPE 0.056\n",
      "    Mean errors by mean parameters (avg link) MSE 0.096 MAE 0.154 MAPE 0.158\n",
      "    Mean errors by mean parameters (route)    MSE 2.391 MAE 0.056 MAPE 0.056\n",
      "Times for model (seconds)\n",
      "[ 586.  863. 1283.  576.  484.  849.]\n",
      "\n",
      "Bayesian net with TiedLSTM cell and 10 lags.Building net...\n",
      "  Tied Cell has been built (in: 16 ) (out: 16 )\n",
      "  Output layer has been built (in: 16 ) (out: 1 )\n",
      "   Partition 1/6 Training on weeks [1 2 3 4] validating training on week 5 Early stopping after 14 epochs\n",
      "      Testing on week 6\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.968, MIL (0.99) 78.487 ICP (0.95) 0.950, MIL (0.95) 65.794 ICP (0.90) 0.925, MIL (0.90) 56.624 ICP (0.80) 0.867, MIL (0.80) 44.759 ICP (0.60) 0.703, MIL (0.60) 29.738 ICP (0.40) 0.495, MIL (0.40) 18.629 ICP (0.20) 0.254, MIL (0.20) 9.014\n",
      "    Quantile errors (route)    ICP (0.99) 0.974, MIL (0.99) 440.028 ICP (0.95) 0.957, MIL (0.95) 367.685 ICP (0.90) 0.930, MIL (0.90) 316.264 ICP (0.80) 0.869, MIL (0.80) 249.755 ICP (0.60) 0.697, MIL (0.60) 165.485 ICP (0.40) 0.502, MIL (0.40) 103.931 ICP (0.20) 0.276, MIL (0.20) 50.380\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.06 (0.975): 0.11 (0.05): 0.10 (0.95): 0.17 (0.1): 0.17 (0.9): 0.25 (0.2): 0.25 (0.8): 0.35 (0.3): 0.31 (0.7): 0.40 (0.4): 0.36 (0.6): 0.41 \n",
      "    Mean errors by sampling (avg link) MSE 0.087 MAE 0.154 MAPE 0.153\n",
      "    Mean errors by sampling (route)    MSE 2.194 MAE 0.820 MAPE 0.051\n",
      "    Mean errors by mean parameters (avg link) MSE 0.086 MAE 0.151 MAPE 0.151\n",
      "    Mean errors by mean parameters (route)    MSE 2.217 MAE 0.051 MAPE 0.051\n",
      "   Partition 2/6 Training on weeks [2 3 4 5] validating training on week 6 Early stopping after 9 epochs\n",
      "      Testing on week 7\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.964, MIL (0.99) 78.423 ICP (0.95) 0.945, MIL (0.95) 65.701 ICP (0.90) 0.920, MIL (0.90) 56.619 ICP (0.80) 0.858, MIL (0.80) 44.803 ICP (0.60) 0.692, MIL (0.60) 29.719 ICP (0.40) 0.478, MIL (0.40) 18.570 ICP (0.20) 0.240, MIL (0.20) 8.985\n",
      "    Quantile errors (route)    ICP (0.99) 0.961, MIL (0.99) 445.958 ICP (0.95) 0.941, MIL (0.95) 373.073 ICP (0.90) 0.918, MIL (0.90) 321.644 ICP (0.80) 0.827, MIL (0.80) 254.520 ICP (0.60) 0.659, MIL (0.60) 168.756 ICP (0.40) 0.436, MIL (0.40) 105.498 ICP (0.20) 0.215, MIL (0.20) 51.335\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.07 (0.025): 0.06 (0.975): 0.13 (0.05): 0.10 (0.95): 0.19 (0.1): 0.17 (0.9): 0.27 (0.2): 0.26 (0.8): 0.37 (0.3): 0.32 (0.7): 0.42 (0.4): 0.37 (0.6): 0.43 \n",
      "    Mean errors by sampling (avg link) MSE 0.127 MAE 0.159 MAPE 0.160\n",
      "    Mean errors by sampling (route)    MSE 3.238 MAE 0.980 MAPE 0.059\n",
      "    Mean errors by mean parameters (avg link) MSE 0.125 MAE 0.155 MAPE 0.157\n",
      "    Mean errors by mean parameters (route)    MSE 3.258 MAE 0.058 MAPE 0.058\n",
      "   Partition 3/6 Training on weeks [3 4 5 6] validating training on week 7 Early stopping after 63 epochs\n",
      "      Testing on week 8\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.968, MIL (0.99) 74.321 ICP (0.95) 0.950, MIL (0.95) 62.152 ICP (0.90) 0.922, MIL (0.90) 53.480 ICP (0.80) 0.858, MIL (0.80) 42.304 ICP (0.60) 0.695, MIL (0.60) 28.043 ICP (0.40) 0.490, MIL (0.40) 17.516 ICP (0.20) 0.250, MIL (0.20) 8.434\n",
      "    Quantile errors (route)    ICP (0.99) 0.972, MIL (0.99) 421.680 ICP (0.95) 0.961, MIL (0.95) 353.692 ICP (0.90) 0.933, MIL (0.90) 303.903 ICP (0.80) 0.870, MIL (0.80) 239.297 ICP (0.60) 0.697, MIL (0.60) 159.705 ICP (0.40) 0.497, MIL (0.40) 99.779 ICP (0.20) 0.250, MIL (0.20) 48.280\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.06 (0.975): 0.11 (0.05): 0.10 (0.95): 0.16 (0.1): 0.17 (0.9): 0.24 (0.2): 0.26 (0.8): 0.34 (0.3): 0.32 (0.7): 0.39 (0.4): 0.36 (0.6): 0.40 \n",
      "    Mean errors by sampling (avg link) MSE 0.101 MAE 0.148 MAPE 0.151\n",
      "    Mean errors by sampling (route)    MSE 2.887 MAE 0.826 MAPE 0.052\n",
      "    Mean errors by mean parameters (avg link) MSE 0.098 MAE 0.145 MAPE 0.148\n",
      "    Mean errors by mean parameters (route)    MSE 2.638 MAE 0.050 MAPE 0.050\n",
      "   Partition 4/6 Training on weeks [4 5 6 7] validating training on week 8 Early stopping after 38 epochs\n",
      "      Testing on week 9\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.961, MIL (0.99) 66.272 ICP (0.95) 0.937, MIL (0.95) 55.582 ICP (0.90) 0.907, MIL (0.90) 47.880 ICP (0.80) 0.831, MIL (0.80) 37.860 ICP (0.60) 0.654, MIL (0.60) 25.141 ICP (0.40) 0.453, MIL (0.40) 15.708 ICP (0.20) 0.229, MIL (0.20) 7.605\n",
      "    Quantile errors (route)    ICP (0.99) 0.949, MIL (0.99) 337.113 ICP (0.95) 0.900, MIL (0.95) 281.121 ICP (0.90) 0.865, MIL (0.90) 241.615 ICP (0.80) 0.781, MIL (0.80) 190.741 ICP (0.60) 0.599, MIL (0.60) 126.672 ICP (0.40) 0.403, MIL (0.40) 79.226 ICP (0.20) 0.205, MIL (0.20) 38.486\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.05 (0.975): 0.10 (0.05): 0.09 (0.95): 0.15 (0.1): 0.15 (0.9): 0.22 (0.2): 0.24 (0.8): 0.31 (0.3): 0.30 (0.7): 0.36 (0.4): 0.35 (0.6): 0.38 \n",
      "    Mean errors by sampling (avg link) MSE 0.094 MAE 0.147 MAPE 0.149\n",
      "    Mean errors by sampling (route)    MSE 2.121 MAE 0.820 MAPE 0.050\n",
      "    Mean errors by mean parameters (avg link) MSE 0.092 MAE 0.145 MAPE 0.147\n",
      "    Mean errors by mean parameters (route)    MSE 2.009 MAE 0.049 MAPE 0.049\n",
      "   Partition 5/6 Training on weeks [5 6 7 8] validating training on week 9 Early stopping after 19 epochs\n",
      "      Testing on week 10\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.952, MIL (0.99) 65.354 ICP (0.95) 0.924, MIL (0.95) 54.672 ICP (0.90) 0.891, MIL (0.90) 47.074 ICP (0.80) 0.815, MIL (0.80) 37.196 ICP (0.60) 0.627, MIL (0.60) 24.665 ICP (0.40) 0.423, MIL (0.40) 15.415 ICP (0.20) 0.213, MIL (0.20) 7.427\n",
      "    Quantile errors (route)    ICP (0.99) 0.922, MIL (0.99) 329.112 ICP (0.95) 0.883, MIL (0.95) 276.447 ICP (0.90) 0.840, MIL (0.90) 239.120 ICP (0.80) 0.741, MIL (0.80) 189.035 ICP (0.60) 0.559, MIL (0.60) 125.359 ICP (0.40) 0.377, MIL (0.40) 78.504 ICP (0.20) 0.185, MIL (0.20) 37.646\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.19 (0.025): 0.06 (0.975): 0.24 (0.05): 0.10 (0.95): 0.28 (0.1): 0.17 (0.9): 0.35 (0.2): 0.28 (0.8): 0.44 (0.3): 0.36 (0.7): 0.48 (0.4): 0.43 (0.6): 0.49 \n",
      "    Mean errors by sampling (avg link) MSE 0.239 MAE 0.151 MAPE 0.156\n",
      "    Mean errors by sampling (route)    MSE 13.511 MAE 0.857 MAPE 0.059\n",
      "    Mean errors by mean parameters (avg link) MSE 0.231 MAE 0.149 MAPE 0.154\n",
      "    Mean errors by mean parameters (route)    MSE 12.612 MAE 0.059 MAPE 0.059\n",
      "   Partition 6/6 Training on weeks [6 7 8 9] validating training on week 10 Early stopping after 31 epochs\n",
      "      Testing on week 11\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.959, MIL (0.99) 67.380 ICP (0.95) 0.932, MIL (0.95) 56.491 ICP (0.90) 0.897, MIL (0.90) 48.683 ICP (0.80) 0.821, MIL (0.80) 38.532 ICP (0.60) 0.637, MIL (0.60) 25.525 ICP (0.40) 0.435, MIL (0.40) 15.982 ICP (0.20) 0.223, MIL (0.20) 7.750\n",
      "    Quantile errors (route)    ICP (0.99) 0.942, MIL (0.99) 341.632 ICP (0.95) 0.895, MIL (0.95) 285.727 ICP (0.90) 0.846, MIL (0.90) 246.020 ICP (0.80) 0.751, MIL (0.80) 194.404 ICP (0.60) 0.584, MIL (0.60) 128.959 ICP (0.40) 0.405, MIL (0.40) 80.109 ICP (0.20) 0.199, MIL (0.20) 39.085\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.05 (0.975): 0.11 (0.05): 0.09 (0.95): 0.16 (0.1): 0.16 (0.9): 0.23 (0.2): 0.25 (0.8): 0.33 (0.3): 0.32 (0.7): 0.38 (0.4): 0.37 (0.6): 0.40 \n",
      "    Mean errors by sampling (avg link) MSE 0.095 MAE 0.156 MAPE 0.159\n",
      "    Mean errors by sampling (route)    MSE 2.219 MAE 0.846 MAPE 0.054\n",
      "    Mean errors by mean parameters (avg link) MSE 0.094 MAE 0.154 MAPE 0.158\n",
      "    Mean errors by mean parameters (route)    MSE 2.200 MAE 0.054 MAPE 0.054\n",
      "Times for model (seconds)\n",
      "[ 242.  162. 1005.  618.  323.  516.]\n",
      "\n",
      "Bayesian net with TiedLSTM cell and 20 lags.Building net...\n",
      "  Tied Cell has been built (in: 16 ) (out: 16 )\n",
      "  Output layer has been built (in: 16 ) (out: 1 )\n",
      "   Partition 1/6 Training on weeks [1 2 3 4] validating training on week 5 Early stopping after 13 epochs\n",
      "      Testing on week 6\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.967, MIL (0.99) 77.854 ICP (0.95) 0.949, MIL (0.95) 65.171 ICP (0.90) 0.927, MIL (0.90) 56.123 ICP (0.80) 0.869, MIL (0.80) 44.401 ICP (0.60) 0.701, MIL (0.60) 29.521 ICP (0.40) 0.488, MIL (0.40) 18.466 ICP (0.20) 0.256, MIL (0.20) 8.949\n",
      "    Quantile errors (route)    ICP (0.99) 0.972, MIL (0.99) 431.921 ICP (0.95) 0.948, MIL (0.95) 363.251 ICP (0.90) 0.918, MIL (0.90) 313.563 ICP (0.80) 0.863, MIL (0.80) 248.452 ICP (0.60) 0.699, MIL (0.60) 164.901 ICP (0.40) 0.484, MIL (0.40) 103.051 ICP (0.20) 0.266, MIL (0.20) 50.443\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.06 (0.975): 0.11 (0.05): 0.10 (0.95): 0.17 (0.1): 0.16 (0.9): 0.25 (0.2): 0.25 (0.8): 0.35 (0.3): 0.32 (0.7): 0.40 (0.4): 0.36 (0.6): 0.41 \n",
      "    Mean errors by sampling (avg link) MSE 0.089 MAE 0.156 MAPE 0.154\n",
      "    Mean errors by sampling (route)    MSE 2.343 MAE 0.862 MAPE 0.052\n",
      "    Mean errors by mean parameters (avg link) MSE 0.087 MAE 0.151 MAPE 0.151\n",
      "    Mean errors by mean parameters (route)    MSE 2.312 MAE 0.051 MAPE 0.051\n",
      "   Partition 2/6 Training on weeks [2 3 4 5] validating training on week 6 Early stopping after 12 epochs\n",
      "      Testing on week 7\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.963, MIL (0.99) 80.449 ICP (0.95) 0.947, MIL (0.95) 67.481 ICP (0.90) 0.925, MIL (0.90) 58.117 ICP (0.80) 0.862, MIL (0.80) 45.922 ICP (0.60) 0.702, MIL (0.60) 30.439 ICP (0.40) 0.494, MIL (0.40) 19.115 ICP (0.20) 0.252, MIL (0.20) 9.213\n",
      "    Quantile errors (route)    ICP (0.99) 0.964, MIL (0.99) 481.814 ICP (0.95) 0.948, MIL (0.95) 404.179 ICP (0.90) 0.931, MIL (0.90) 349.317 ICP (0.80) 0.856, MIL (0.80) 275.937 ICP (0.60) 0.692, MIL (0.60) 183.932 ICP (0.40) 0.482, MIL (0.40) 116.113 ICP (0.20) 0.248, MIL (0.20) 56.212\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.07 (0.025): 0.06 (0.975): 0.13 (0.05): 0.11 (0.95): 0.19 (0.1): 0.17 (0.9): 0.27 (0.2): 0.26 (0.8): 0.38 (0.3): 0.32 (0.7): 0.43 (0.4): 0.37 (0.6): 0.44 \n",
      "    Mean errors by sampling (avg link) MSE 0.126 MAE 0.158 MAPE 0.159\n",
      "    Mean errors by sampling (route)    MSE 3.323 MAE 0.959 MAPE 0.059\n",
      "    Mean errors by mean parameters (avg link) MSE 0.124 MAE 0.154 MAPE 0.157\n",
      "    Mean errors by mean parameters (route)    MSE 3.258 MAE 0.059 MAPE 0.059\n",
      "   Partition 3/6 Training on weeks [3 4 5 6] validating training on week 7 Early stopping after 45 epochs\n",
      "      Testing on week 8\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.966, MIL (0.99) 72.436 ICP (0.95) 0.945, MIL (0.95) 60.625 ICP (0.90) 0.918, MIL (0.90) 52.228 ICP (0.80) 0.853, MIL (0.80) 41.339 ICP (0.60) 0.688, MIL (0.60) 27.431 ICP (0.40) 0.488, MIL (0.40) 17.150 ICP (0.20) 0.254, MIL (0.20) 8.293\n",
      "    Quantile errors (route)    ICP (0.99) 0.959, MIL (0.99) 386.184 ICP (0.95) 0.941, MIL (0.95) 324.670 ICP (0.90) 0.908, MIL (0.90) 280.306 ICP (0.80) 0.833, MIL (0.80) 221.538 ICP (0.60) 0.668, MIL (0.60) 146.017 ICP (0.40) 0.455, MIL (0.40) 91.551 ICP (0.20) 0.235, MIL (0.20) 44.284\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.06 (0.975): 0.11 (0.05): 0.10 (0.95): 0.16 (0.1): 0.16 (0.9): 0.24 (0.2): 0.26 (0.8): 0.34 (0.3): 0.32 (0.7): 0.38 (0.4): 0.36 (0.6): 0.40 \n",
      "    Mean errors by sampling (avg link) MSE 0.101 MAE 0.147 MAPE 0.150\n",
      "    Mean errors by sampling (route)    MSE 2.872 MAE 0.783 MAPE 0.051\n",
      "    Mean errors by mean parameters (avg link) MSE 0.097 MAE 0.145 MAPE 0.148\n",
      "    Mean errors by mean parameters (route)    MSE 2.631 MAE 0.050 MAPE 0.050\n",
      "   Partition 4/6 Training on weeks [4 5 6 7] validating training on week 8 Early stopping after 23 epochs\n",
      "      Testing on week 9\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.964, MIL (0.99) 67.535 ICP (0.95) 0.940, MIL (0.95) 56.554 ICP (0.90) 0.910, MIL (0.90) 48.747 ICP (0.80) 0.835, MIL (0.80) 38.561 ICP (0.60) 0.660, MIL (0.60) 25.628 ICP (0.40) 0.459, MIL (0.40) 16.032 ICP (0.20) 0.229, MIL (0.20) 7.750\n",
      "    Quantile errors (route)    ICP (0.99) 0.950, MIL (0.99) 349.721 ICP (0.95) 0.915, MIL (0.95) 294.671 ICP (0.90) 0.872, MIL (0.90) 253.637 ICP (0.80) 0.783, MIL (0.80) 200.212 ICP (0.60) 0.594, MIL (0.60) 132.141 ICP (0.40) 0.398, MIL (0.40) 82.611 ICP (0.20) 0.200, MIL (0.20) 40.065\n",
      " Tilted loss for quantile:(0.005): 0.01 (0.995): 0.06 (0.025): 0.05 (0.975): 0.10 (0.05): 0.09 (0.95): 0.15 (0.1): 0.15 (0.9): 0.22 (0.2): 0.24 (0.8): 0.31 (0.3): 0.30 (0.7): 0.36 (0.4): 0.35 (0.6): 0.38 \n",
      "    Mean errors by sampling (avg link) MSE 0.094 MAE 0.149 MAPE 0.150\n",
      "    Mean errors by sampling (route)    MSE 2.277 MAE 0.862 MAPE 0.052\n",
      "    Mean errors by mean parameters (avg link) MSE 0.093 MAE 0.149 MAPE 0.148\n",
      "    Mean errors by mean parameters (route)    MSE 2.231 MAE 0.051 MAPE 0.051\n",
      "   Partition 5/6 Training on weeks [5 6 7 8] validating training on week 9 Early stopping after 17 epochs\n",
      "      Testing on week 10\n",
      "    Quantile errors (avg link)  ICP (0.99) 0.949, MIL (0.99) 66.177 ICP (0.95) 0.923, MIL (0.95) 55.409 ICP (0.90) 0.891, MIL (0.90) 47.787 ICP (0.80) 0.819, MIL (0.80) 37.796 ICP (0.60) 0.634, MIL (0.60) 25.004 ICP (0.40) 0.425, MIL (0.40) 15.604 ICP (0.20) 0.210, MIL (0.20) 7.538\n",
      "    Quantile errors (route)    ICP (0.99) 0.921, MIL (0.99) 339.570 ICP (0.95) 0.885, MIL (0.95) 284.586 ICP (0.90) 0.839, MIL (0.90) 245.790 ICP (0.80) 0.739, MIL (0.80) 194.684 ICP (0.60) 0.565, MIL (0.60) 128.460 ICP (0.40) 0.371, MIL (0.40) 80.779 ICP (0.20) 0.184, MIL (0.20) 38.855\n",
      " Tilted loss for quantile:(0.005): 0.02 (0.995): 0.18 (0.025): 0.06 (0.975): 0.23 (0.05): 0.10 (0.95): 0.28 (0.1): 0.17 (0.9): 0.35 (0.2): 0.28 (0.8): 0.44 (0.3): 0.36 (0.7): 0.48 (0.4): 0.43 (0.6): 0.49 \n",
      "    Mean errors by sampling (avg link) MSE 0.241 MAE 0.152 MAPE 0.157\n",
      "    Mean errors by sampling (route)    MSE 13.617 MAE 0.885 MAPE 0.062\n",
      "    Mean errors by mean parameters (avg link) MSE 0.227 MAE 0.151 MAPE 0.156\n",
      "    Mean errors by mean parameters (route)    MSE 12.152 MAE 0.061 MAPE 0.061\n",
      "   Partition 6/6 Training on weeks [6 7 8 9] validating training on week 10 Early stopping after 25 epochs\n",
      "      Testing on week 11\n"
     ]
    }
   ],
   "source": [
    "mse = np.empty((num_partitions, len(mtypes)))\n",
    "mae = np.empty((num_partitions, len(mtypes)))\n",
    "mape = np.empty((num_partitions, len(mtypes)))\n",
    "mse2 = np.empty((num_partitions, len(mtypes)))\n",
    "mae2 = np.empty((num_partitions, len(mtypes)))\n",
    "mape2 = np.empty((num_partitions, len(mtypes)))\n",
    "icp = np.empty((int(len(quantiles)/2), num_partitions, len(mtypes)))\n",
    "mil = np.empty((int(len(quantiles)/2), num_partitions, len(mtypes)))\n",
    "tradeoff = np.empty((int(len(quantiles)/2), num_partitions, len(mtypes)))\n",
    "tilt_loss = np.empty((len(quantiles), num_partitions, len(mtypes)))\n",
    "time = np.empty((num_partitions, len(mtypes)))\n",
    "\n",
    "icp_route = np.empty((int(len(quantiles)/2), num_partitions, len(mtypes)))\n",
    "mil_route = np.empty((int(len(quantiles)/2), num_partitions, len(mtypes)))\n",
    "tradeoff_route = np.empty((int(len(quantiles)/2), num_partitions, len(mtypes)))\n",
    "mse_route = np.empty((num_partitions, len(mtypes)))\n",
    "mae_route = np.empty((num_partitions, len(mtypes)))\n",
    "mape_route = np.empty((num_partitions, len(mtypes)))\n",
    "mse_route2 = np.empty((num_partitions, len(mtypes)))\n",
    "mae_route2 = np.empty((num_partitions, len(mtypes)))\n",
    "mape_route2 = np.empty((num_partitions, len(mtypes)))\n",
    "\n",
    "for m, (mtype, units, lags) in enumerate(zip(mtypes, param_lst, lag_lst)):\n",
    "  print(\"Bayesian net with {} cell and {} lags.\".format(mtype, lags), end='')\n",
    "  net = BayesianRNN([units], num_links, batch_size, init, mtype, prior)\n",
    "  for part in range(num_partitions):\n",
    "    print(\"   Partition {}/{}\".format(part+1, num_partitions), end=' ')\n",
    "    train_from = part*advance_days\n",
    "    train_to = period_train_days + part*advance_days\n",
    "    val_to = period_train_days + part*advance_days + period_val_days\n",
    "    test_to = period_train_days + period_val_days + part*advance_days + period_test_days\n",
    "\n",
    "    train_ind = np.arange(train_from, train_to)\n",
    "    val_ind = np.arange(train_to, val_to)\n",
    "    test_ind = np.arange(val_to, test_to)\n",
    "    print(\"Training on weeks {}\".format(np.arange(int((train_ind[6]+1)/7),\n",
    "                                                  int((train_ind[-1]+1)/7)+1)), end=' ')\n",
    "    print(\"validating training on week {}\".format(int((val_ind[-1]+1)/7)), end=' ')\n",
    "    keep_train = range(int(2297920*train_ind[ 0]/period), int(2297920*train_ind[-1]/period))\n",
    "    keep_val = range(int(2297920*train_ind[-1]/period)+1, int(2297920*val_ind[-1]/period))\n",
    "    keep_test = range(int(2297920*val_ind[-1]/period)+1, int(2297920*test_ind[-1]/period))\n",
    "\n",
    "    ## Load the part of the dataset we need for training, validation, testing\n",
    "    data_train = pd.read_csv('data/link_travel_time_local.csv.gz', compression='gzip', \n",
    "                              parse_dates = True, index_col = 0,\n",
    "                              skiprows = lambda x: skip_row(x, keep_train))\n",
    "    data_val = pd.read_csv('data/link_travel_time_local.csv.gz', compression='gzip', \n",
    "                              parse_dates = True, index_col = 0,\n",
    "\t\t\t\t\t\t\t\t              skiprows = lambda x: skip_row(x, keep_val))\n",
    "    data_test  = pd.read_csv('data/link_travel_time_local.csv.gz', compression='gzip',\n",
    "                              parse_dates = True, index_col = 0,\n",
    "                              skiprows = lambda x: skip_row(x, keep_test))\n",
    "    ## Sort data by links and add categorical columns TOD, Weekday\n",
    "    data_train, order = sort_and_order(data_train)\n",
    "    data_val, order = sort_and_order(data_val)\n",
    "    data_test, order = sort_and_order(data_test)\n",
    "    \n",
    "    ## Transform datasets using the mean and std for train and val set.\n",
    "    means_df_train, scales_df_train = fit_scale(pd.concat([data_train,data_val]), order)\n",
    "    ts_train_df, mean_train_df, scale_train_df = transform(data_train, \n",
    "                                                            means_df_train, \n",
    "                                                            scales_df_train, \n",
    "                                                            order,\n",
    "                                                            freq = '15min')\n",
    "    ts_val_df, mean_val_df, scale_val_df = transform(data_val, \n",
    "                                                      means_df_train, \n",
    "                                                      scales_df_train, \n",
    "                                                      order,\n",
    "                                                      freq = '15min')\n",
    "    ts_test_df, mean_test_df, scale_test_df = transform(data_test, \n",
    "                                                        means_df_train, \n",
    "                                                        scales_df_train, \n",
    "                                                        order,\n",
    "                                                        freq = '15min')\n",
    "    ## Roll data into timeseries format\n",
    "    X_train, y_train, y_ix_train, y_mean_train, y_std_train = roll(ts_train_df.index, \n",
    "                                                                    ts_train_df.values,\n",
    "                                                                    mean_train_df.values,\n",
    "                                                                    scale_train_df.values,\n",
    "                                                                    lags, \n",
    "                                                                    preds)\n",
    "    X_val, y_val, y_ix_val, y_mean_val, y_std_val = roll(ts_val_df.index, \n",
    "                                                        ts_val_df.values,\n",
    "                                                        mean_val_df.values,\n",
    "                                                        scale_val_df.values,\n",
    "                                                        lags, \n",
    "                                                        preds)\n",
    "    X_test, y_test, y_ix_test, y_mean_test, y_std_test = roll(ts_test_df.index, \n",
    "                                                              ts_test_df.values, \n",
    "                                                              mean_test_df.values,\n",
    "                                                              scale_test_df.values,\n",
    "                                                              lags, \n",
    "                                                              preds)\n",
    "    num_batch_train = int(X_train.shape[0]/batch_size)\n",
    "    num_batch_test = int(X_test.shape[0]/batch_size)\n",
    "\n",
    "    data_train = tf.data.Dataset.from_tensor_slices((X_train, \n",
    "                                                    y_train)).shuffle(1000).batch(batch_size, drop_remainder=True)\n",
    "    data_test = tf.data.Dataset.from_tensor_slices((X_test, \n",
    "                                                    y_test)).batch(batch_size, drop_remainder=True)\n",
    "    drop_train = len(y_train) - num_batch_train*batch_size\n",
    "    drop_test = len(y_test) - num_batch_test*batch_size\n",
    "    X_train, y_train, y_ix_train, y_mean_train, y_std_train = drop_remainder(X_train, y_train, y_ix_train, y_mean_train, y_std_train, drop_train)\n",
    "    X_test, y_test, y_ix_test, y_mean_test, y_std_test = drop_remainder(X_test, y_test, y_ix_test, y_mean_test, y_std_test, drop_test)\n",
    "        \n",
    "    t1 = datetime.now()\n",
    "    ## Initialise weights using last partition\n",
    "    if part > 0:\n",
    "      init_lr = 1e-2\n",
    "      net.set_weights(best_weights)\n",
    "\n",
    "    best_elbo = 10000000000000000000000\n",
    "    best_mse = 10000000000000000000000\n",
    "    lr = init_lr\n",
    "    for epoch in range(epochs):\n",
    "      ## Training\n",
    "      elbo_sum, mse_sum = train_step(net, data_train, lr)\n",
    "      mse_avg_train = mse_sum.numpy() / num_batch_train\n",
    "      elbo_avg = elbo_sum.numpy() / num_batch_train\n",
    "      ## Validation\n",
    "      elbo_test, mse_sum = val_loss(net, data_test)\n",
    "      mse_avg_test = mse_sum.numpy() / num_batch_test\n",
    "      elbo_avg_test = elbo_test.numpy() / num_batch_test\n",
    "      \n",
    "      if elbo_avg_test < best_elbo:\n",
    "          best_weights = net.get_weights()\n",
    "          best_elbo = elbo_avg_test\n",
    "          patience_counter = 0\n",
    "      elif mse_avg_test < best_mse:\n",
    "          best_weights = net.get_weights()\n",
    "          best_mse = mse_avg_test\n",
    "          patience_counter = 0\n",
    "      patience_counter += 1\n",
    "      \n",
    "      if patience_counter > patience:\n",
    "          print(\"Early stopping after {} epochs\".format(epoch))\n",
    "          break\n",
    "      if patience_counter >= int(patience/2):\n",
    "          lr = init_lr*np.exp(-0.1*epoch)\n",
    "    ## Set weights back to the best model\n",
    "    net.set_weights(best_weights)\n",
    "\n",
    "    t2 = datetime.now()\n",
    "    time[part, m] = (t2-t1).seconds\n",
    "\n",
    "    print(\"      Testing on week {}\".format(int((test_ind[-1]+1)/7)))\n",
    "    nsamples = 50\n",
    "    y_pred = np.empty((nsamples, y_test.shape[0], y_test.shape[2]))\n",
    "    y_pred_mean_param = np.empty((y_test.shape[0], y_test.shape[2]))\n",
    "    T = 0\n",
    "    for x_batch, _ in data_test:\n",
    "        x = tf.cast(x_batch,tf.float32)\n",
    "        y_pred_mean_param[T*batch_size:(T+1)*batch_size] = net(x, training=False, sampling=False)\n",
    "        for samp in range(nsamples):\n",
    "          out = net(x, training=False, sampling=True)\n",
    "          y_pred[samp, T*batch_size:(T+1)*batch_size] = tfd.MultivariateNormalTriL(out, scale_tril=net.y_std).sample()\n",
    "        T = T+1\n",
    "    y_pred_all = np.sum(y_pred*y_std_test[:,0] + y_mean_test[:,0], axis=2)\n",
    "    y_pred_mean_param = y_pred_mean_param*y_std_test[:,0,:] + y_mean_test[:,0,:]\n",
    "    for i in range(int(len(quantiles)/2)):\n",
    "      icp_lnks = np.zeros(num_links)\n",
    "      mil_lnks = np.zeros(num_links) \n",
    "      tradeoff_lnks = np.zeros(num_links)  \n",
    "      for lnk in range(num_links):\n",
    "        q1 = np.quantile(y_pred[:,:,lnk], quantiles[2*i],   axis=0)\n",
    "        q2 = np.quantile(y_pred[:,:,lnk], quantiles[2*i+1], axis=0)\n",
    "        q1_back = q1*y_std_test[:,0,lnk] + y_mean_test[:,0,lnk]\n",
    "        q2_back = q2*y_std_test[:,0,lnk] + y_mean_test[:,0,lnk]\n",
    "        icp_lnks[lnk] = 1-(np.sum(y_test[:,0,lnk] < q1)+np.sum(y_test[:,0,lnk] > q2))/len(y_test)\n",
    "        mil_lnks[lnk] = np.sum(np.maximum(0, q2_back - q1_back)) / len(y_test)\n",
    "        tradeoff_lnks[lnk] = np.abs(icp_lnks[lnk] - pred_ints[i])*mil_lnks[lnk]\n",
    "      icp[i, part, m] = np.mean(icp_lnks)\n",
    "      mil[i, part, m] = np.mean(mil_lnks)\n",
    "      tradeoff[i, part, m] = np.mean(tradeoff)\n",
    "\n",
    "    mse_lnks = np.zeros(num_links)\n",
    "    mae_lnks = np.zeros(num_links) \n",
    "    mape_lnks = np.zeros(num_links)\n",
    "    mse_lnks2 = np.zeros(num_links)\n",
    "    mae_lnks2 = np.zeros(num_links) \n",
    "    mape_lnks2 = np.zeros(num_links)\n",
    "    for lnk in range(num_links):\n",
    "      y_mean_lnk = np.mean(y_pred[:,:,lnk], axis=0)*y_std_test[:,0,lnk] + y_mean_test[:,0,lnk]\n",
    "      y_true_lnk = y_test[:,0,lnk]*y_std_test[:,0,lnk] + y_mean_test[:,0,lnk]\n",
    "      mse_lnks[lnk] = eval_mse(y_mean_lnk/60, y_true_lnk/60)\n",
    "      mae_lnks[lnk] = eval_mae(y_mean_lnk/60, y_true_lnk/60)\n",
    "      mape_lnks[lnk] = np.mean(np.abs((y_mean_lnk/60 - y_true_lnk/60)/(y_true_lnk/60)))\n",
    "      mse_lnks2[lnk] = eval_mse(y_pred_mean_param[:,lnk]/60, y_true_lnk/60)\n",
    "      mae_lnks2[lnk] = eval_mae(y_pred_mean_param[:,lnk]/60, y_true_lnk/60)\n",
    "      mape_lnks2[lnk] = np.mean(np.abs((y_pred_mean_param[:,lnk]/60 - y_true_lnk/60)/(y_true_lnk/60)))\n",
    "    mse[part, m] = np.mean(mse_lnks)\n",
    "    mae[part, m] = np.mean(mae_lnks)\n",
    "    mape[part, m] = np.mean(mape_lnks)\n",
    "    mse2[part, m] = np.mean(mse_lnks2)\n",
    "    mae2[part, m] = np.mean(mae_lnks2)\n",
    "    mape2[part, m] = np.mean(mape_lnks2)\n",
    "    for q, quan in enumerate(quantiles):\n",
    "      quan_pred = np.quantile(y_pred, quan, axis=0)\n",
    "      tilt_loss[q, part, m] = tilted_loss_np_t(quan, np.squeeze(y_test), quan_pred)\n",
    "    \n",
    "    y_test_all = np.sum(y_test[:,0]*y_std_test[:,0] + y_mean_test[:,0], axis=1)\n",
    "    for i in range(int(len(quantiles)/2)):\n",
    "      q1_all = np.quantile(y_pred_all, quantiles[2*i],   axis=0)\n",
    "      q2_all = np.quantile(y_pred_all, quantiles[2*i+1], axis=0)\n",
    "      icp_route[i, part, m] = 1-(np.sum(y_test_all < q1_all)+np.sum(y_test_all > q2_all))/len(y_test)\n",
    "      mil_route[i, part, m] = np.sum(np.maximum(0, q2_all - q1_all)) / len(y_test)\n",
    "      tradeoff_route[i, part, m] = np.abs(icp_route[i, part, m] - pred_ints[i])*mil_route[i, part, m]\n",
    "\n",
    "    y_mean_all = np.mean(y_pred_all, axis=0)\n",
    "    mse_route[part, m] = eval_mse(y_mean_all/60, y_test_all/60)\n",
    "    mape_route[part, m] = np.mean(np.abs((y_mean_all/60 - y_test_all/60)/(y_test_all/60)))\n",
    "    mae_route[part, m] = eval_mae(y_mean_all/60, y_test_all/60)\n",
    "    mse_route2[part, m] = eval_mse(np.sum(y_pred_mean_param,axis=1)/60, y_test_all/60)\n",
    "    mape_route2[part, m] = np.mean(np.abs((np.sum(y_pred_mean_param,axis=1)/60 - y_test_all/60)/(y_test_all/60)))\n",
    "    mae_route2[part, m] = eval_mae(np.sum(y_pred_mean_param,axis=1)/60, y_test_all/60)\n",
    "\n",
    "    ## Print results\n",
    "    print(\"    Quantile errors (avg link)  ICP (0.99) %.3f, MIL (0.99) %.3f\" % (icp[0,part,m], mil[0,part,m]), end = ' ')\n",
    "    print(\"ICP (0.95) %.3f, MIL (0.95) %.3f\" % (icp[1,part,m], mil[1,part,m]), end = ' ')\n",
    "    print(\"ICP (0.90) %.3f, MIL (0.90) %.3f\" % (icp[2,part,m], mil[2,part,m]), end = ' ')\n",
    "    print(\"ICP (0.80) %.3f, MIL (0.80) %.3f\" % (icp[3,part,m], mil[3,part,m]), end = ' ')\n",
    "    print(\"ICP (0.60) %.3f, MIL (0.60) %.3f\" % (icp[4,part,m], mil[4,part,m]), end = ' ')\n",
    "    print(\"ICP (0.40) %.3f, MIL (0.40) %.3f\" % (icp[5,part,m], mil[5,part,m]), end = ' ')\n",
    "    print(\"ICP (0.20) %.3f, MIL (0.20) %.3f\" % (icp[6,part,m], mil[6,part,m]))\n",
    "\n",
    "    print(\"    Quantile errors (route)    ICP (0.99) %.3f, MIL (0.99) %.3f\" % (icp_route[0,part,m], mil_route[0,part,m]), end = ' ')\n",
    "    print(\"ICP (0.95) %.3f, MIL (0.95) %.3f\" % (icp_route[1,part,m], mil_route[1,part,m]), end = ' ')\n",
    "    print(\"ICP (0.90) %.3f, MIL (0.90) %.3f\" % (icp_route[2,part,m], mil_route[2,part,m]), end = ' ')\n",
    "    print(\"ICP (0.80) %.3f, MIL (0.80) %.3f\" % (icp_route[3,part,m], mil_route[3,part,m]), end = ' ')\n",
    "    print(\"ICP (0.60) %.3f, MIL (0.60) %.3f\" % (icp_route[4,part,m], mil_route[4,part,m]), end = ' ')\n",
    "    print(\"ICP (0.40) %.3f, MIL (0.40) %.3f\" % (icp_route[5,part,m], mil_route[5,part,m]), end = ' ')\n",
    "    print(\"ICP (0.20) %.3f, MIL (0.20) %.3f\" % (icp_route[6,part,m], mil_route[6,part,m]))\n",
    "\n",
    "    print(\" Tilted loss for quantile:\", end='')\n",
    "    for q, quan in enumerate(quantiles):\n",
    "      print(\"({}): \".format(quan), end='')\n",
    "      print(\"%.2f \" % tilt_loss[q,part,m], end = '')\n",
    "    print(\"\")\n",
    "    print(\"    Mean errors by sampling (avg link) MSE %.3f MAE %.3f MAPE %.3f\" % (mse[part,m], mae[part,m], mape[part,m]))\n",
    "    print(\"    Mean errors by sampling (route)    MSE %.3f MAE %.3f MAPE %.3f\" % (mse_route[part,m], mae_route[part,m], mape_route[part,m]))\n",
    "    print(\"    Mean errors by mean parameters (avg link) MSE %.3f MAE %.3f MAPE %.3f\" % (mse2[part,m], mae2[part,m], mape2[part,m]))\n",
    "    print(\"    Mean errors by mean parameters (route)    MSE %.3f MAE %.3f MAPE %.3f\" % (mse_route2[part,m], mape_route2[part,m], mape_route2[part,m]))\n",
    "  print(\"Times for model (seconds)\")\n",
    "  print(time[:,m])\n",
    "  print(\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BRNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
