{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfVCzuUaUnuu",
        "colab_type": "code",
        "outputId": "5e34ba4e-abba-4fd9-cbe5-efe58534298c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/andersparslov/BayesianNN-DQR.git\n",
        "import os\n",
        "os.chdir('BayesianNN-DQR')\n",
        "\n",
        "from scipy.stats import norm\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "from sklearn.metrics import mean_absolute_error as eval_mae\n",
        "from sklearn.metrics import mean_squared_error as eval_mse\n",
        "from sklearn import linear_model\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Lambda\n",
        "np.seterr(divide='ignore')\n",
        "\n",
        "def sort_and_order(data):\n",
        "  ## Sort links by order \n",
        "  data, order = sort_links(data, '1973:1412', '7057:7058')\n",
        "  ## Make a link order column e.g here the neighbouring links for link 1 are 0 and 2.\n",
        "  data.loc[:,'link_order'] = data['link_ref'].astype('category')\n",
        "  not_in_list = data['link_order'].cat.categories.difference(order)\n",
        "  data.loc[:,'link_order'] = data['link_order'].cat.set_categories(np.hstack((order, not_in_list)), ordered=True)\n",
        "  data.loc[:,'link_order'] = data['link_order'].cat.codes\n",
        "  ## Add week of day column [Monday, ..., Sunday] = [0, ..., 6]\n",
        "  data.loc[:, 'Weekday'] = data.index.weekday\n",
        "  ## Add hour of the time to dataframe\n",
        "  data.loc[:, 'Hour'] = data.index.hour\n",
        "  ## Add time of day variables to data frame\n",
        "  data.loc[:, 'TOD'] = data.Hour.apply(tod_interval)\n",
        "  data = data.sort_values('link_order')\n",
        "  return data, order\n",
        "\n",
        "def skip_row(index, keep_list):\n",
        "\tif (index == 0):\n",
        "\t\treturn False ## Never want to skip the header\n",
        "\treturn index not in keep_list\n",
        "\n",
        "def write_3d(X, filename):\n",
        "  X_list = X.tolist()\n",
        "  with open(filename+'.csv', 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile, delimiter=',')\n",
        "      writer.writerows(X_list)\n",
        "\n",
        "def skip_row(index, keep_list):\n",
        "\tif (index == 0):\n",
        "\t\treturn False ## Never want to skip the header\n",
        "\treturn index not in keep_list\n",
        "\n",
        "def transform(data, means_df, scales_df, order, freq = '15min'):\n",
        "  tss = { }\n",
        "  ws = { }\n",
        "  removed_mean = { }\n",
        "  removed_scale = { }\n",
        "  lnk_list = []\n",
        "  for lnk, data_link in data.groupby('link_ref', sort = False):\n",
        "      # Link Data Time Indexed\n",
        "      link_time_ix = pd.DatetimeIndex(data_link.index)\n",
        "      data_link = data_link.set_index(link_time_ix)\n",
        "      # Link Reference Data Index\n",
        "      ix_week = data_link['Weekday'].tolist()\n",
        "      ix_tod = data_link['TOD'].tolist()\n",
        "      ## Create multi index for the two lists\n",
        "      mult_ind = pd.MultiIndex.from_arrays([ix_week, ix_tod])\n",
        "\n",
        "      link_travel_time_k = data_link['link_travel_time'].resample(freq).mean()\n",
        "      removed_mean[lnk] = pd.Series(data=means_df[lnk].loc[mult_ind].values, \n",
        "                                    index = link_time_ix).resample(freq).mean()\n",
        "      removed_scale[lnk] = pd.Series(data =scales_df[lnk].loc[mult_ind].values, \n",
        "                                      index = link_time_ix).resample(freq).mean()\n",
        "      tss[lnk] = (link_travel_time_k - removed_mean[lnk].values) / removed_scale[lnk].values\n",
        "      ws[lnk] = data_link['link_travel_time'].resample(freq).count()\n",
        "      lnk_list.append(lnk)\n",
        "\n",
        "  ts = pd.DataFrame(data = tss).fillna(method='pad').fillna(0) \n",
        "  df_removed_mean = pd.DataFrame(data = removed_mean, index = ts.index).fillna(method='pad').fillna(method='bfill') \n",
        "  df_removed_scale = pd.DataFrame(data = removed_scale, index = ts.index).fillna(method='pad').fillna(method='bfill')    \n",
        "  w = pd.DataFrame(data = ws).fillna(0) # Link Travel Time Weights, e.g. number of measurements\n",
        "  return ts[order], df_removed_mean[order], df_removed_scale[order]\n",
        "\n",
        "def fit_scale(data, order, ref_freq = '15min'):\n",
        "  means = { }\n",
        "  scales = { }\n",
        "  low = { }\n",
        "  upr = { }\n",
        "\n",
        "  grouping = data[data['link_travel_time'].notnull()].groupby('link_ref', sort = False)\n",
        "  for link_ref, data_link in grouping:\n",
        "      # Fit outlier bounds using MAD\n",
        "      median = data_link.groupby('Weekday')['link_travel_time'].median()\n",
        "      error = pd.concat([data_link['Weekday'], np.abs(data_link['link_travel_time'] - median[data_link['Weekday']].values)], axis = 1)\n",
        "      mad = 1.4826 * error.groupby('Weekday')['link_travel_time'].median()\n",
        "      _low = median - 3 * mad\n",
        "      _upr = median + 3 * mad\n",
        "      mask = (_low[data_link['Weekday']].values < data_link['link_travel_time']) & (data_link['link_travel_time'] < _upr[data_link['Weekday']].values)\n",
        "      data_link_no = data_link[mask]\n",
        "      _mean = data_link_no.groupby(['Weekday', 'TOD'])['link_travel_time'].mean()\n",
        "      means[link_ref] = _mean\n",
        "      scale = data_link_no.groupby(['Weekday', 'TOD'])['link_travel_time'].std()\n",
        "      scales[link_ref] = scale\n",
        "\n",
        "      low[link_ref] = _low\n",
        "      upr[link_ref] = _upr\n",
        "\n",
        "  means_df = pd.DataFrame(data=means).interpolate()\n",
        "  scales_df = pd.DataFrame(data=scales).interpolate()\n",
        "  low_df = pd.DataFrame(data=low).interpolate()\n",
        "  upr_df = pd.DataFrame(data=upr).interpolate()\n",
        "\n",
        "  ## Correct order of links\n",
        "  means_df = means_df[order]\n",
        "  scales_df = scales_df[order]\n",
        "  low_df = low_df[order]\n",
        "  upr_df = upr_df[order]\n",
        "\n",
        "  # Fill NaNs    \n",
        "  means_df = means_df.fillna(method='pad').fillna(method='bfill')\n",
        "  scales_df = scales_df.fillna(method='pad').fillna(method='bfill')\n",
        "  low_df = low_df.fillna(method='pad').fillna(method='bfill')\n",
        "  upr_df = upr_df.fillna(method='pad').fillna(method='bfill')\n",
        "  \n",
        "  return means_df, scales_df\n",
        "\n",
        "def roll(ix, ts, removed_mean, removed_scale, lags, preds):\n",
        "  X = np.stack([np.roll(ts, i, axis = 0) for i in range(lags, 0, -1)], axis = 1)[lags:-preds,]\n",
        "  Y = np.stack([np.roll(ts, -i, axis = 0) for i in range(0, preds, 1)], axis = 1)[lags:-preds,]\n",
        "  Y_ix = ix[lags:-preds]\n",
        "  Y_mean = np.stack([np.roll(removed_mean, -i, axis = 0) for i in range(0, preds, 1)], axis = 1)[lags:-preds,]\n",
        "  Y_scale = np.stack([np.roll(removed_scale, -i, axis = 0) for i in range(0, preds, 1)], axis = 1)[lags:-preds,]\n",
        "  return X, Y, Y_ix, Y_mean, Y_scale\n",
        "\n",
        "def sort_links(data, start_link, end_link):\n",
        "  ordered_list = [start_link]\n",
        "  links = data.loc[:,'link_ref'].unique()\n",
        "  stop_end = start_link.rpartition(':')[2]\n",
        "  while True:\n",
        "      stop_start = stop_end\n",
        "      for lnk in links:\n",
        "          if(lnk.rpartition(':')[0] == stop_start):\n",
        "              if( (lnk in ordered_list) or (lnk == end_link) ):\n",
        "                  break\n",
        "              else:\n",
        "                  ordered_list.append(lnk)\n",
        "                  stop_end = lnk.rpartition(':')[2]\n",
        "      if(stop_start == stop_end):\n",
        "          break\n",
        "  ordered_list.append(end_link)\n",
        "  ## Only include links in ordered list.\n",
        "  data = data[data.loc[:,'link_ref'].isin(ordered_list)]\n",
        "  return data, ordered_list\n",
        "\n",
        "def tod_interval(x):\n",
        "  if(x < 2):\n",
        "      return 0\n",
        "  elif(x < 4):\n",
        "      return 1\n",
        "  elif(x < 6):\n",
        "      return 2\n",
        "  elif(x < 8):\n",
        "      return 3\n",
        "  elif(x < 10):\n",
        "      return 4\n",
        "  elif(x < 12):\n",
        "      return 5\n",
        "  elif(x < 14):\n",
        "      return 6\n",
        "  elif(x < 16):\n",
        "      return 7\n",
        "  elif(x < 18):\n",
        "      return 8\n",
        "  elif(x < 20):\n",
        "      return 9\n",
        "  elif(x < 22):\n",
        "      return 10\n",
        "  elif(x < 24):\n",
        "      return 11\n",
        "\n",
        "def split_df(data, start_train, end_train, end_test):\n",
        "  data_train = data.loc[start_train:end_train]\n",
        "  data_test = data.loc[end_train:end_test]\n",
        "  return data_train, data_test\n",
        "\n",
        "def split_df_with_val(data, start_train, end_train, end_val, end_test):\n",
        "  data_train = data.loc[start_train:end_train]\n",
        "  data_val = data.loc[end_train:end_val]\n",
        "  data_test = data.loc[end_val:end_test]\n",
        "  return data_train, data_val, data_test\n",
        "\n",
        "def drop_remainder(X, y, y_ix, y_mean, y_std, drop):\n",
        "  return X[:-drop], y[:-drop], y_ix[:-drop], y_mean[:-drop], y_std[:-drop]\n",
        "\n",
        "def tilted_loss_np_t(q, y, f):\n",
        "  e = y-f\n",
        "  # The term inside k.mean is a one line simplification of the first equation\n",
        "  return np.mean(np.maximum(q*e, (q-1)*e))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BayesianNN-DQR'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/128)\u001b[K\rremote: Counting objects:   1% (2/128)\u001b[K\rremote: Counting objects:   2% (3/128)\u001b[K\rremote: Counting objects:   3% (4/128)\u001b[K\rremote: Counting objects:   4% (6/128)\u001b[K\rremote: Counting objects:   5% (7/128)\u001b[K\rremote: Counting objects:   6% (8/128)\u001b[K\rremote: Counting objects:   7% (9/128)\u001b[K\rremote: Counting objects:   8% (11/128)\u001b[K\rremote: Counting objects:   9% (12/128)\u001b[K\rremote: Counting objects:  10% (13/128)\u001b[K\rremote: Counting objects:  11% (15/128)\u001b[K\rremote: Counting objects:  12% (16/128)\u001b[K\rremote: Counting objects:  13% (17/128)\u001b[K\rremote: Counting objects:  14% (18/128)\u001b[K\rremote: Counting objects:  15% (20/128)\u001b[K\rremote: Counting objects:  16% (21/128)\u001b[K\rremote: Counting objects:  17% (22/128)\u001b[K\rremote: Counting objects:  18% (24/128)\u001b[K\rremote: Counting objects:  19% (25/128)\u001b[K\rremote: Counting objects:  20% (26/128)\u001b[K\rremote: Counting objects:  21% (27/128)\u001b[K\rremote: Counting objects:  22% (29/128)\u001b[K\rremote: Counting objects:  23% (30/128)\u001b[K\rremote: Counting objects:  24% (31/128)\u001b[K\rremote: Counting objects:  25% (32/128)\u001b[K\rremote: Counting objects:  26% (34/128)\u001b[K\rremote: Counting objects:  27% (35/128)\u001b[K\rremote: Counting objects:  28% (36/128)\u001b[K\rremote: Counting objects:  29% (38/128)\u001b[K\rremote: Counting objects:  30% (39/128)\u001b[K\rremote: Counting objects:  31% (40/128)\u001b[K\rremote: Counting objects:  32% (41/128)\u001b[K\rremote: Counting objects:  33% (43/128)\u001b[K\rremote: Counting objects:  34% (44/128)\u001b[K\rremote: Counting objects:  35% (45/128)\u001b[K\rremote: Counting objects:  36% (47/128)\u001b[K\rremote: Counting objects:  37% (48/128)\u001b[K\rremote: Counting objects:  38% (49/128)\u001b[K\rremote: Counting objects:  39% (50/128)\u001b[K\rremote: Counting objects:  40% (52/128)\u001b[K\rremote: Counting objects:  41% (53/128)\u001b[K\rremote: Counting objects:  42% (54/128)\u001b[K\rremote: Counting objects:  43% (56/128)\u001b[K\rremote: Counting objects:  44% (57/128)\u001b[K\rremote: Counting objects:  45% (58/128)\u001b[K\rremote: Counting objects:  46% (59/128)\u001b[K\rremote: Counting objects:  47% (61/128)\u001b[K\rremote: Counting objects:  48% (62/128)\u001b[K\rremote: Counting objects:  49% (63/128)\u001b[K\rremote: Counting objects:  50% (64/128)\u001b[K\rremote: Counting objects:  51% (66/128)\u001b[K\rremote: Counting objects:  52% (67/128)\u001b[K\rremote: Counting objects:  53% (68/128)\u001b[K\rremote: Counting objects:  54% (70/128)\u001b[K\rremote: Counting objects:  55% (71/128)\u001b[K\rremote: Counting objects:  56% (72/128)\u001b[K\rremote: Counting objects:  57% (73/128)\u001b[K\rremote: Counting objects:  58% (75/128)\u001b[K\rremote: Counting objects:  59% (76/128)\u001b[K\rremote: Counting objects:  60% (77/128)\u001b[K\rremote: Counting objects:  61% (79/128)\u001b[K\rremote: Counting objects:  62% (80/128)\u001b[K\rremote: Counting objects:  63% (81/128)\u001b[K\rremote: Counting objects:  64% (82/128)\u001b[K\rremote: Counting objects:  65% (84/128)\u001b[K\rremote: Counting objects:  66% (85/128)\u001b[K\rremote: Counting objects:  67% (86/128)\u001b[K\rremote: Counting objects:  68% (88/128)\u001b[K\rremote: Counting objects:  69% (89/128)\u001b[K\rremote: Counting objects:  70% (90/128)\u001b[K\rremote: Counting objects:  71% (91/128)\u001b[K\rremote: Counting objects:  72% (93/128)\u001b[K\rremote: Counting objects:  73% (94/128)\u001b[K\rremote: Counting objects:  74% (95/128)\u001b[K\rremote: Counting objects:  75% (96/128)\u001b[K\rremote: Counting objects:  76% (98/128)\u001b[K\rremote: Counting objects:  77% (99/128)\u001b[K\rremote: Counting objects:  78% (100/128)\u001b[K\rremote: Counting objects:  79% (102/128)\u001b[K\rremote: Counting objects:  80% (103/128)\u001b[K\rremote: Counting objects:  81% (104/128)\u001b[K\rremote: Counting objects:  82% (105/128)\u001b[K\rremote: Counting objects:  83% (107/128)\u001b[K\rremote: Counting objects:  84% (108/128)\u001b[K\rremote: Counting objects:  85% (109/128)\u001b[K\rremote: Counting objects:  86% (111/128)\u001b[K\rremote: Counting objects:  87% (112/128)\u001b[K\rremote: Counting objects:  88% (113/128)\u001b[K\rremote: Counting objects:  89% (114/128)\u001b[K\rremote: Counting objects:  90% (116/128)\u001b[K\rremote: Counting objects:  91% (117/128)\u001b[K\rremote: Counting objects:  92% (118/128)\u001b[K\rremote: Counting objects:  93% (120/128)\u001b[K\rremote: Counting objects:  94% (121/128)\u001b[K\rremote: Counting objects:  95% (122/128)\u001b[K\rremote: Counting objects:  96% (123/128)\u001b[K\rremote: Counting objects:  97% (125/128)\u001b[K\rremote: Counting objects:  98% (126/128)\u001b[K\rremote: Counting objects:  99% (127/128)\u001b[K\rremote: Counting objects: 100% (128/128)\u001b[K\rremote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 1765 (delta 84), reused 0 (delta 0), pack-reused 1637\u001b[K\n",
            "Receiving objects: 100% (1765/1765), 1.29 GiB | 40.54 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n",
            "Checking out files: 100% (1143/1143), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvG1p3qUVJKE",
        "colab_type": "code",
        "outputId": "38ab0198-fd7f-4b4b-c95a-04706e9153de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/andersparslov/BayesianNN-DQR.git\n",
        "import os\n",
        "os.chdir('BayesianNN-DQR')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BayesianNN-DQR'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 1738 (delta 68), reused 0 (delta 0), pack-reused 1637\u001b[K\n",
            "Receiving objects: 100% (1738/1738), 1.29 GiB | 33.02 MiB/s, done.\n",
            "Resolving deltas: 100% (481/481), done.\n",
            "Checking out files: 100% (1142/1142), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_ELdtK1ypHu",
        "colab_type": "code",
        "outputId": "e895de06-66f5-4cc7-c77b-5ab1915c47e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvkuA0EitYnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, ConvLSTM2D, Flatten, RepeatVector, Reshape, TimeDistributed\n",
        "\n",
        "## Encoder-decoder LSTM for mean \n",
        "def convLstm(num_filters, kernel_length, input_timesteps, num_links, output_timesteps, prob, loss):\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization(name = 'batch_norm_0', input_shape = (input_timesteps, num_links, 1, 1)))\n",
        "\n",
        "    model.add(ConvLSTM2D(name ='conv_lstm_1',\n",
        "                         filters = num_filters, kernel_size = (kernel_length, 1), \n",
        "                         padding='same',\n",
        "                         return_sequences = False))\n",
        "    model.add(Lambda(lambda x: K.dropout(x, level=prob)))\n",
        "    model.add(BatchNormalization(name = 'batch_norm_1'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(RepeatVector(output_timesteps))\n",
        "    model.add(Reshape((output_timesteps, num_links, 1, num_filters)))\n",
        "\n",
        "    model.add(ConvLSTM2D(name ='conv_lstm_2',\n",
        "                         filters = num_filters, kernel_size = (kernel_length, 1), \n",
        "                         padding='same',\n",
        "                         return_sequences = True))\n",
        "    model.add(Lambda(lambda x: K.dropout(x, level=prob)))\n",
        "    model.add(TimeDistributed(Dense(units = 1, name = 'dense_1')))\n",
        "    model.compile(loss = loss, optimizer = 'nadam')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fNn3mW7G0e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Mean parameters (for the selected lag 32 independent mean net)\n",
        "num_units_mean_l10 = 24\n",
        "kern_mean_l10 = 11\n",
        "lags = 10\n",
        "preds = 1\n",
        "num_links = 16\n",
        "prob = 0.20\n",
        "NUM_MC_SAMPLES = 100\n",
        "loss = lambda y,f: mse_loss(y, f)\n",
        "net = convLstm(num_units_mean_l10, kern_mean_l10, lags, num_links, preds, prob, loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q961VGuBXQ4V",
        "colab_type": "code",
        "outputId": "b7ca81c2-190c-40cd-b5d4-dde7018afe7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = datetime.strptime('19/01/21', \"%y/%m/%d\")\n",
        "end   = datetime.strptime('19/04/14', \"%y/%m/%d\")\n",
        "period = (end - start).days\n",
        "period_train_days = 7*4  ## Train on 4 weeks\n",
        "period_val_days = 7 \n",
        "period_test_days =  7    ## Test on  1 week\n",
        "advance_days = 7         ## Advance by 1 week\n",
        "num_partitions = int((period-period_train_days-period_test_days)/advance_days)\n",
        "preds = 1\n",
        "quantiles = np.array([0.025, 0.975, 0.05, 0.95]) \n",
        "pred_ints = np.array([0.95,         0.90])\n",
        "num_links = 16\n",
        "\n",
        "icp_lr = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "mil_lr = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "tradeoff_lr = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "icp2_lr = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "mil2_lr = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "tradeoff2_lr = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "\n",
        "mse_lr = np.empty((preds, num_partitions))\n",
        "mape_lr = np.empty((preds, num_partitions))\n",
        "mse2_lr = np.empty((preds, num_partitions)) ## <- using mean \n",
        "mse2 = np.empty((preds, num_partitions)) ## <- using median\n",
        "mape2_lr = np.empty((preds, num_partitions))\n",
        "\n",
        "icp_mc_a = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "mil_mc_a = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "tradeoff_mc_a = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "icp_mc_s = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "mil_mc_s = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "tradeoff_mc_s = np.empty((preds, int(len(quantiles)/2), num_partitions))\n",
        "mse_mc = np.empty((preds, num_partitions))\n",
        "mape_mc = np.empty((preds, num_partitions))\n",
        "\n",
        "for part in range(num_partitions):\n",
        "  print(\"Partition\", part+1)\n",
        "  train_from = part*advance_days\n",
        "  train_to = period_train_days + part*advance_days\n",
        "  val_to = period_train_days + part*advance_days + period_val_days\n",
        "  test_to = period_train_days + part*advance_days + period_test_days\n",
        "\n",
        "  train_ind = np.arange(train_from, train_to)\n",
        "  val_ind = np.arange(train_to, val_to)\n",
        "  test_ind = np.arange(train_to, test_to)\n",
        "  print(\"Training on weeks {}\".format(np.arange(int((train_ind[6]+1)/7),\n",
        "                                                int((train_ind[-1]+1)/7)+1)), end=' ')\n",
        "  keep_train = range(int(2297920*train_ind[ 0]/period), int(2297920*train_ind[-1]/period))\n",
        "  keep_val = range(int(2297920*train_ind[-1]/period)+1, int(2297920*val_ind[-1]/period))\n",
        "  keep_test = range(int(2297920*train_ind[-1]/period)+1, int(2297920*test_ind[-1]/period))\n",
        "\n",
        "  ## Load the part of the dataset we need for training, validation, testing\n",
        "  data_train = pd.read_csv('data/link_travel_time_local.csv.gz', compression='gzip', \n",
        "                            parse_dates = True, index_col = 0,\n",
        "                            skiprows = lambda x: skip_row(x, keep_train))\n",
        "  data_val = pd.read_csv('data/link_travel_time_local.csv.gz', compression='gzip', \n",
        "                          parse_dates = True, index_col = 0,\n",
        "                          skiprows = lambda x: skip_row(x, keep_val))\n",
        "  data_test  = pd.read_csv('data/link_travel_time_local.csv.gz', compression='gzip',\n",
        "                            parse_dates = True, index_col = 0,\n",
        "                            skiprows = lambda x: skip_row(x, keep_test))\n",
        "  ## Sort data by links and add categorical columns TOD, Weekday\n",
        "  data_train, order = sort_and_order(data_train)\n",
        "  data_val, order = sort_and_order(data_val)\n",
        "  data_test, order = sort_and_order(data_test)\n",
        "\n",
        "  ## Transform datasets using the mean and std for train and val set.\n",
        "  #means_df_train, scales_df_train = fit_scale(pd.concat([data_train,data_val]), order)\n",
        "  means_df_train, scales_df_train = fit_scale(data_train, order)\n",
        "  ts_train_df, mean_train_df, scale_train_df = transform(data_train, \n",
        "                                                          means_df_train, \n",
        "                                                          scales_df_train, \n",
        "                                                          order,\n",
        "                                                          freq = '15min')\n",
        "  ts_val_df, mean_val_df, scale_val_df = transform(data_val, \n",
        "                                                  means_df_train, \n",
        "                                                  scales_df_train, \n",
        "                                                  order,\n",
        "                                                  freq = '15min')\n",
        "  ts_test_df, mean_test_df, scale_test_df = transform(data_test, \n",
        "                                                      means_df_train, \n",
        "                                                      scales_df_train, \n",
        "                                                      order,\n",
        "                                                      freq = '15min')\n",
        "  ## Roll data into timeseries format\n",
        "  X_train, y_train, y_ix_train, y_mean_train, y_std_train = roll(ts_train_df.index, \n",
        "                                                                  ts_train_df.values,\n",
        "                                                                  mean_train_df.values,\n",
        "                                                                  scale_train_df.values,\n",
        "                                                                  lags, \n",
        "                                                                  preds)\n",
        "  X_val, y_val, y_ix_val, y_mean_val, y_std_val = roll(ts_val_df.index, \n",
        "                                                    ts_val_df.values,\n",
        "                                                    mean_val_df.values,\n",
        "                                                    scale_val_df.values,\n",
        "                                                    lags, \n",
        "                                                    preds)\n",
        "  X_test, y_test, y_ix_test, y_mean_test, y_std_test = roll(ts_test_df.index, \n",
        "                                                            ts_test_df.values, \n",
        "                                                            mean_test_df.values,\n",
        "                                                            scale_test_df.values,\n",
        "                                                            lags, \n",
        "                                                            preds)\n",
        "  T1 = y_ix_train[0]\n",
        "  ix_train = np.array((y_ix_train-T1).seconds/60)[:,np.newaxis]\n",
        "  ix_test  = np.array((y_ix_test-T1).seconds/60)[:,np.newaxis]\n",
        "  col_names = [\"l%d\" % (i,) for i in range(lags)]\n",
        "  col_names.append(\"y\")\n",
        "  qr_str = \"y ~ \" + ''.join([\"+l%d\" % (x,) for x in range(lags)])[1:]\n",
        "\n",
        "  \n",
        "\n",
        "  print(\"Fitting Linear Quantile Regression\")\n",
        "  for t in range(preds):\n",
        "    print(\"t + {}\".format(t+1))\n",
        "\n",
        "    y_pred_lr = np.empty((y_test.shape[0], preds, num_links))\n",
        "    y_pred = np.empty((y_test.shape[0], preds, num_links))\n",
        "    y_pred_q = np.empty((len(quantiles), y_test.shape[0], preds, num_links))\n",
        "    icp_lnks = np.empty((preds, num_links, int(len(quantiles)/2)))\n",
        "    mil_lnks = np.empty((preds, num_links, int(len(quantiles)/2)))\n",
        "    tradeoff_lnks = np.empty((preds, num_links, int(len(quantiles)/2)))\n",
        "    mse_lnks = np.empty((preds, num_links, int(len(quantiles)/2)))\n",
        "    mape_lnks = np.empty((preds, num_links, int(len(quantiles)/2)))\n",
        "\n",
        "    for lnk in range(num_links):\n",
        "      regr = linear_model.LinearRegression()\n",
        "      regr.fit(X_train[:,:,lnk], y_train[:,t,lnk])\n",
        "      y_pred_lr[:,t,lnk] = regr.predict(X_test[:,:,lnk])*y_std_test[:,t,lnk] + y_mean_test[:,t,lnk]\n",
        "      \n",
        "      for q, quan in enumerate(quantiles):\n",
        "        data = pd.DataFrame(data = np.hstack([X_train[:,:,lnk], y_train[:,t,lnk:(lnk+1)]]), columns = col_names)\n",
        "        data_test = pd.DataFrame(data = np.hstack([X_test[:,:,lnk]]), columns = col_names[:lags])\n",
        "        qr = smf.quantreg(qr_str, data)\n",
        "        res = qr.fit(q=quan)\n",
        "        y_pred_q[q,:,t,lnk] = res.predict(data_test)\n",
        "      res = qr.fit(q=0.5)\n",
        "      y_pred[:,t,lnk] = res.predict(data_test)\n",
        "\n",
        "      ## Quantile evaluation\n",
        "      for i in range(int(len(quantiles)/2)):\n",
        "        q1 = y_pred_q[2*i,:,t,lnk]\n",
        "        q2 = y_pred_q[2*i+1,:,t,lnk]\n",
        "        q1_back = q1*y_std_test[:,0,lnk]+y_mean_test[:,0,lnk]\n",
        "        q2_back = q2*y_std_test[:,0,lnk]+y_mean_test[:,0,lnk]\n",
        "        icp_lnks[t, lnk, i] = 1-(np.sum(y_test[:,t,lnk] < q1)+np.sum(y_test[:,t,lnk] > q2))/len(y_test)\n",
        "        mil_lnks[t, lnk, i] = np.sum(np.maximum(0, q2_back - q1_back)) / len(y_test)\n",
        "        tradeoff_lnks[t, lnk, i] = (icp_lnks[t,lnk,i] - pred_ints[i])*mil_lnks[t, lnk, i]\n",
        "      y_test_lnk = y_test[:,t,lnk]*y_std_test[:,t,lnk] + y_mean_test[:,t,lnk]\n",
        "      y_pred_lnk = y_pred[:,t,lnk]*y_std_test[:,t,lnk] + y_mean_test[:,t,lnk]\n",
        "      mse_lnk = eval_mse(y_test_lnk/60, y_pred_lnk/60)\n",
        "      mape_lnk = np.mean(np.abs((y_pred_lnk/60 - y_test_lnk/60)/(y_test_lnk/60)))\n",
        "    \n",
        "    mse_lr[t, part] = np.mean(mse_lnk)\n",
        "    mape_lr[t, part] = np.mean(mape_lnk)\n",
        "    for i in range(int(len(quantiles)/2)):\n",
        "      icp_lr[t, i, part] = np.mean(icp_lnks[t, :, i])\n",
        "      mil_lr[t, i, part] = np.mean(mil_lnks[t, :, i])\n",
        "      tradeoff_lr[t, i, part] = np.mean(tradeoff_lnks[t, :, i])\n",
        "\n",
        "      q1 = y_pred_q[2*i,:,t,:]*y_std_test[:,t,:] + y_mean_test[:,t,:]\n",
        "      q2 = y_pred_q[2*i+1,:,t,:]*y_std_test[:,t,:] + y_mean_test[:,t,:]\n",
        "      q1_all = np.sum(q1, axis=1)\n",
        "      q2_all = np.sum(q2, axis=1)\n",
        "      y_test_all = np.sum(y_test[:,t]*y_std_test[:,t,:] + y_mean_test[:,t,:], axis=1)\n",
        "      icp2_lr[t,  i, part] = 1-(np.sum(y_test_all < q1_all)+np.sum(y_test_all> q2_all))/len(y_test)\n",
        "      mil2_lr[t,  i, part] = np.sum(np.maximum(0, q2 - q1)) / len(y_test)\n",
        "      tradeoff2_lr[t, i, part] = (icp2_lr[t,i,part] - pred_ints[i])*mil2_lr[t,i,part]\n",
        "\n",
        "    y_test_all = np.sum(y_test[:,t]*y_std_test[:,t] + y_mean_test[:,t], axis=1)\n",
        "    y_pred_all = np.sum(y_pred[:,t]*y_std_test[:,t] + y_mean_test[:,t], axis=1)\n",
        "    y_pred_all_lr = np.sum(y_pred_lr[:,t], axis=1)\n",
        "    mse2_lr[t, part] = eval_mse(y_test_all/60, y_pred_all_lr/60)\n",
        "    mse2[t, part] = eval_mse(y_test_all/60, y_pred_all/60)\n",
        "    mape2_lr[t, part] = np.mean(np.abs((y_pred_all/60 - y_test_all/60)/(y_test_all/60)))\n",
        "    \n",
        "    print(\" LR ICP (95%) {} MSE mean (route) MSE med (route) {} MAPE {} (route)\".format(icp_lr[t, 0, part], mse2_lr[t, part], mse2[t, part], mape2_lr[t, part]))\n",
        "    \n",
        "    X_train = X_train[:,:,:,np.newaxis,np.newaxis]\n",
        "    X_val = X_val[:,:,:,np.newaxis,np.newaxis]\n",
        "    X_test = X_test[:,:,:,np.newaxis,np.newaxis]\n",
        "    y_train = y_train[:,:,:,np.newaxis,np.newaxis]\n",
        "    y_val = y_val[:,:,:,np.newaxis,np.newaxis]\n",
        "    y_test = y_test[:,:,:,np.newaxis,np.newaxis]\n",
        "    \n",
        "    print(\"Fitting model for MC Dropout\")\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',  patience=5, mode='min')\n",
        "    check = tf.keras.callbacks.ModelCheckpoint(\"/content/gdrive/My Drive/ModelCheckpoint/baseline_check.hdf5\",  \n",
        "                                               monitor='val_loss', mode='min', \n",
        "                                               save_best_only=True)\n",
        "    net.fit(X_train, y_train, batch_size=80, epochs=50, validation_data=(X_val, y_val), callbacks = [es, check])\n",
        "    net.load_weights(\"/content/gdrive/My Drive/ModelCheckpoint/baseline_check.hdf5\")\n",
        "\n",
        "    print(\"Drawing samples\", end='')\n",
        "    # make predictions\n",
        "    predictions = np.zeros((NUM_MC_SAMPLES, len(X_test), num_links))\n",
        "    for s in range(NUM_MC_SAMPLES):\n",
        "      predictions[s,:,:] = np.squeeze(net.predict(X_test))\n",
        "      if (s+1) % (int(NUM_MC_SAMPLES/3)) == 0:\n",
        "        print(\".\", end = '')\n",
        "    print(\"\")\n",
        "    # make predictions\n",
        "    trues_sigma = y_val[:,0,:,0,0]\n",
        "    predictions_sigma = np.zeros((NUM_MC_SAMPLES, len(X_val), num_links))\n",
        "    for s in range(NUM_MC_SAMPLES):\n",
        "      predictions_sigma[s,:,:] = np.squeeze(net.predict(X_val))\n",
        "\n",
        "    # select optimal value of alpha based on log probability on validation set\n",
        "    alphas = np.zeros((num_links,))\n",
        "    for lnk in range(num_links):\n",
        "      means = np.mean(predictions_sigma[:,:,lnk], axis=0)\n",
        "      vars_ = np.var(predictions_sigma[:,:,lnk], axis=0)\n",
        "      \n",
        "      best_log_prob = -np.inf\n",
        "      for alpha in np.arange(0.0,1.0,0.01):\n",
        "        log_prob = 0.0\n",
        "        for n in range(len(y_val)):\n",
        "          log_prob += np.log(norm.pdf(trues_sigma[n,lnk], loc=means[n], scale=np.sqrt(vars_[n]+alpha)))\n",
        "        if log_prob > best_log_prob:\n",
        "          best_log_prob = log_prob\n",
        "          alphas[lnk] = alpha\n",
        "\n",
        "    y_pred =     np.empty( (y_test.shape[0], num_links) )\n",
        "    y_pred_q =   np.empty( (len(quantiles), y_test.shape[0], num_links) )\n",
        "    icp_lnks_a = np.empty( (preds,num_links,int(len(quantiles)/2))) \n",
        "    mil_lnks_a = np.empty( (preds,num_links,int(len(quantiles)/2)))\n",
        "    tradeoff_lnks_a = np.empty( (preds,num_links,int(len(quantiles)/2)))\n",
        "    icp_lnks_s = np.empty( (preds,num_links,int(len(quantiles)/2)))\n",
        "    mil_lnks_s = np.empty( (preds,num_links,int(len(quantiles)/2)))\n",
        "    tradeoff_lnks_s = np.empty( (preds,num_links,int(len(quantiles)/2)))\n",
        "    for lnk in range(num_links):\n",
        "      for i in range(int(len(quantiles)/2)):\n",
        "        trues = y_test[:,0,lnk,0,0]\n",
        "        preds_mc = np.mean(predictions, axis=0)[:,lnk]\n",
        "        \n",
        "        sigma2_est = np.mean((np.mean(predictions_sigma, axis=0) - trues_sigma)**2, axis=0)\n",
        "        var_dl = np.var(predictions, axis=0)[:,lnk]\n",
        "        \n",
        "        inv_cdf_upr = norm.ppf(quantiles[2*i+1])\n",
        "        inv_cdf_lwr = norm.ppf(quantiles[2*i])\n",
        "        # here we use different alternatives for calibrating the uncertainty estimates produced by MCdropout\n",
        "        q1 = np.mean(predictions, axis=0)[:,lnk] + inv_cdf_lwr*np.sqrt(var_dl + alphas[lnk]) \n",
        "        q2 = np.mean(predictions, axis=0)[:,lnk] + inv_cdf_upr*np.sqrt(var_dl + alphas[lnk])\n",
        "        q1_back = q1*y_std_test[:,0,lnk]+y_mean_test[:,0,lnk]\n",
        "        q2_back = q2*y_std_test[:,0,lnk]+y_mean_test[:,0,lnk]\n",
        "        icp_lnks_a[t,lnk,i] = 1-(np.sum(y_test[:,0,lnk,0,0] < q1) + np.sum(y_test[:,0,lnk,0,0] > q2))/len(y_test)\n",
        "        mil_lnks_a[t,lnk,i] = np.sum(np.maximum(0, q2_back - q1_back))/len(y_test)\n",
        "        tradeoff_lnks_a[t,lnk,i] = (icp_lnks[t,lnk,i]-pred_ints[i])*mil_lnks[t,lnk,i]\n",
        "\n",
        "        q1 = np.mean(predictions, axis=0)[:,i] + inv_cdf_lwr*np.sqrt(var_dl + sigma2_est[i])\n",
        "        q2 = np.mean(predictions, axis=0)[:,i] + inv_cdf_upr*np.sqrt(var_dl + sigma2_est[i])\n",
        "        q1_back = q1*y_std_test[:,0,lnk]+y_mean_test[:,0,lnk]\n",
        "        q2_back = q2*y_std_test[:,0,lnk]+y_mean_test[:,0,lnk]\n",
        "        icp_lnks_s[t,lnk,i] = 1-(np.sum(y_test[:,0,lnk,0,0] < q1) + np.sum(y_test[:,0,lnk,0,0] > q2))/len(y_test)\n",
        "        mil_lnks_s[t,lnk,i] = np.sum(np.maximum(0, q2_back - q1_back))/len(y_test)\n",
        "        tradeoff_lnks_s[t,lnk,i] = (icp_lnks[t,lnk,i]-pred_ints[i])*mil_lnks[t,lnk,i]\n",
        "\n",
        "    for i in range(int(len(quantiles)/2)):\n",
        "      icp_mc_s[t, i, part] = np.mean(icp_lnks_s[t,:,i])\n",
        "      mil_mc_s[t, i, part] = np.mean(mil_lnks_s[t,:,i])\n",
        "      tradeoff_mc_s = np.mean(tradeoff_lnks_s[t,:,i])\n",
        "      icp_mc_a[t, i, part] = np.mean(icp_lnks_a[t,:,i])\n",
        "      mil_mc_a[t, i, part] = np.mean(mil_lnks_a[t,:,i])\n",
        "      tradeoff_mc_a = np.mean(tradeoff_lnks_a[t,:,i])\n",
        "    y_pred_all = np.sum(np.squeeze(np.mean(predictions, axis=0))*y_std_test[:,t] + y_mean_test[:,t], axis=1)\n",
        "    y_test_all = np.sum(y_test[:,t,:,0,0]*y_std_test[:,t] + y_mean_test[:,t], axis=1)\n",
        "    mse_mc[t,part] = eval_mse(y_test_all/60, y_pred_all/60)\n",
        "    mape_mc[t,part] = np.sum(np.abs((y_pred_all/60 - y_test_all/60)/(y_test_all/60)))/len(y_test)\n",
        "    print(\" MC ICP [alpha] (95%) {} MSE (route) {} MAPE {} (route)\".format(icp_mc_a[t, 0, part], mse_mc[t, part], mape_mc[t, part]))\n",
        "    print(\" MC ICP [sigma] (95%) {} MSE (route) {} MAPE {} (route)\".format(icp_mc_s[t, 0, part], mse_mc[t, part], mape_mc[t, part]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Partition 1\n",
            "Training on weeks [1 2 3 4] Fitting Linear Quantile Regression\n",
            "t + 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " LR ICP (95%) 0.932542872454448 MSE mean (route) MSE med (route) 1.3163599680929685 MAPE 1.5115572216742754 (route)\n",
            "Fitting model for MC Dropout\n",
            "Train on 3652 samples, validate on 933 samples\n",
            "Epoch 1/50\n",
            "3652/3652 [==============================] - 13s 4ms/sample - loss: 2.4542 - val_loss: 1.0443\n",
            "Epoch 2/50\n",
            "3652/3652 [==============================] - 9s 3ms/sample - loss: 2.2764 - val_loss: 1.0384\n",
            "Epoch 3/50\n",
            "3652/3652 [==============================] - 10s 3ms/sample - loss: 2.2287 - val_loss: 1.0277\n",
            "Epoch 4/50\n",
            "3652/3652 [==============================] - 10s 3ms/sample - loss: 2.1974 - val_loss: 1.0223\n",
            "Epoch 5/50\n",
            "3652/3652 [==============================] - 10s 3ms/sample - loss: 2.1529 - val_loss: 1.0100\n",
            "Epoch 6/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 2.1285 - val_loss: 0.9970\n",
            "Epoch 7/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 2.0892 - val_loss: 0.9903\n",
            "Epoch 8/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 2.0864 - val_loss: 0.9736\n",
            "Epoch 9/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 2.0606 - val_loss: 0.9666\n",
            "Epoch 10/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 2.0494 - val_loss: 0.9670\n",
            "Epoch 11/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 2.0133 - val_loss: 0.9706\n",
            "Epoch 12/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 1.9970 - val_loss: 0.9644\n",
            "Epoch 13/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 1.9745 - val_loss: 0.9718\n",
            "Epoch 14/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 1.9665 - val_loss: 0.9729\n",
            "Epoch 15/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 1.9637 - val_loss: 0.9686\n",
            "Epoch 16/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 1.9391 - val_loss: 0.9702\n",
            "Epoch 17/50\n",
            "3652/3652 [==============================] - 11s 3ms/sample - loss: 1.9316 - val_loss: 0.9717\n",
            "Drawing samples...\n",
            " MC ICP [alpha] (95%) 0.9480171489817792 MSE (route) 1.307535083759884 MAPE 0.04384689074207308 (route)\n",
            " MC ICP [sigma] (95%) 0.947414255091104 MSE (route) 1.307535083759884 MAPE 0.04384689074207308 (route)\n",
            "Partition 2\n",
            "Training on weeks [2 3 4 5] Fitting Linear Quantile Regression\n",
            "t + 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " LR ICP (95%) 0.9317208904109588 MSE mean (route) MSE med (route) 1.537182268263054 MAPE 1.8892116493708175 (route)\n",
            "Fitting model for MC Dropout\n",
            "Train on 3607 samples, validate on 876 samples\n",
            "Epoch 1/50\n",
            "3607/3607 [==============================] - 9s 3ms/sample - loss: 2.0791 - val_loss: 1.1297\n",
            "Epoch 2/50\n",
            "3607/3607 [==============================] - 9s 2ms/sample - loss: 2.0685 - val_loss: 1.1311\n",
            "Epoch 3/50\n",
            "3607/3607 [==============================] - 9s 3ms/sample - loss: 2.0272 - val_loss: 1.1287\n",
            "Epoch 4/50\n",
            "3607/3607 [==============================] - 9s 2ms/sample - loss: 2.0288 - val_loss: 1.1264\n",
            "Epoch 5/50\n",
            "3607/3607 [==============================] - 9s 2ms/sample - loss: 2.0121 - val_loss: 1.1277\n",
            "Epoch 6/50\n",
            "3607/3607 [==============================] - 9s 2ms/sample - loss: 1.9853 - val_loss: 1.1290\n",
            "Epoch 7/50\n",
            "3607/3607 [==============================] - 9s 2ms/sample - loss: 1.9781 - val_loss: 1.1408\n",
            "Epoch 8/50\n",
            "3607/3607 [==============================] - 9s 2ms/sample - loss: 1.9794 - val_loss: 1.1361\n",
            "Epoch 9/50\n",
            "3607/3607 [==============================] - 9s 2ms/sample - loss: 1.9696 - val_loss: 1.1309\n",
            "Drawing samples...\n",
            " MC ICP [alpha] (95%) 0.950699200913242 MSE (route) 1.5316219231439776 MAPE 0.04429296152241398 (route)\n",
            " MC ICP [sigma] (95%) 0.948416095890411 MSE (route) 1.5316219231439776 MAPE 0.04429296152241398 (route)\n",
            "Partition 3\n",
            "Training on weeks [3 4 5 6] Fitting Linear Quantile Regression\n",
            "t + 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " LR ICP (95%) 0.9291530944625407 MSE mean (route) MSE med (route) 2.32447062391702 MAPE 2.6956942996558886 (route)\n",
            "Fitting model for MC Dropout\n",
            "Train on 3559 samples, validate on 921 samples\n",
            "Epoch 1/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1674 - val_loss: 1.3227\n",
            "Epoch 2/50\n",
            "3559/3559 [==============================] - 9s 3ms/sample - loss: 1.1522 - val_loss: 1.3156\n",
            "Epoch 3/50\n",
            "3559/3559 [==============================] - 9s 3ms/sample - loss: 1.1580 - val_loss: 1.3114\n",
            "Epoch 4/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1498 - val_loss: 1.3215\n",
            "Epoch 5/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1539 - val_loss: 1.3205\n",
            "Epoch 6/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1346 - val_loss: 1.3121\n",
            "Epoch 7/50\n",
            "3559/3559 [==============================] - 9s 3ms/sample - loss: 1.1458 - val_loss: 1.3081\n",
            "Epoch 8/50\n",
            "3559/3559 [==============================] - 9s 3ms/sample - loss: 1.1368 - val_loss: 1.3042\n",
            "Epoch 9/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1426 - val_loss: 1.3100\n",
            "Epoch 10/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1313 - val_loss: 1.3072\n",
            "Epoch 11/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1308 - val_loss: 1.3134\n",
            "Epoch 12/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1258 - val_loss: 1.3163\n",
            "Epoch 13/50\n",
            "3559/3559 [==============================] - 9s 2ms/sample - loss: 1.1344 - val_loss: 1.3150\n",
            "Drawing samples...\n",
            " MC ICP [alpha] (95%) 0.9545331161780672 MSE (route) 2.305176750958098 MAPE 0.0511560245967908 (route)\n",
            " MC ICP [sigma] (95%) 0.9316639522258414 MSE (route) 2.305176750958098 MAPE 0.0511560245967908 (route)\n",
            "Partition 4\n",
            "Training on weeks [4 5 6 7] Fitting Linear Quantile Regression\n",
            "t + 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " LR ICP (95%) 0.9311827956989247 MSE mean (route) MSE med (route) 2.6810476263629934 MAPE 3.3799560689022266 (route)\n",
            "Fitting model for MC Dropout\n",
            "Train on 3606 samples, validate on 930 samples\n",
            "Epoch 1/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.1080 - val_loss: 1.4169\n",
            "Epoch 2/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.1107 - val_loss: 1.4094\n",
            "Epoch 3/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.1058 - val_loss: 1.4000\n",
            "Epoch 4/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.1047 - val_loss: 1.4030\n",
            "Epoch 5/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0977 - val_loss: 1.3920\n",
            "Epoch 6/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0892 - val_loss: 1.4108\n",
            "Epoch 7/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.1003 - val_loss: 1.4080\n",
            "Epoch 8/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.1033 - val_loss: 1.3926\n",
            "Epoch 9/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0861 - val_loss: 1.4157\n",
            "Epoch 10/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0901 - val_loss: 1.3842\n",
            "Epoch 11/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0899 - val_loss: 1.4084\n",
            "Epoch 12/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0894 - val_loss: 1.4119\n",
            "Epoch 13/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0891 - val_loss: 1.3976\n",
            "Epoch 14/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0831 - val_loss: 1.4254\n",
            "Epoch 15/50\n",
            "3606/3606 [==============================] - 10s 3ms/sample - loss: 1.0828 - val_loss: 1.4083\n",
            "Drawing samples...\n",
            " MC ICP [alpha] (95%) 0.9486559139784946 MSE (route) 2.796241896145413 MAPE 0.04895144143489274 (route)\n",
            " MC ICP [sigma] (95%) 0.9346774193548386 MSE (route) 2.796241896145413 MAPE 0.04895144143489274 (route)\n",
            "Partition 5\n",
            "Training on weeks [5 6 7 8] Fitting Linear Quantile Regression\n",
            "t + 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " LR ICP (95%) 0.9336890243902438 MSE mean (route) MSE med (route) 1.9604162083942078 MAPE 2.310349081893488 (route)\n",
            "Fitting model for MC Dropout\n",
            "Train on 3626 samples, validate on 902 samples\n",
            "Epoch 1/50\n",
            "3626/3626 [==============================] - 9s 3ms/sample - loss: 1.1376 - val_loss: 1.3067\n",
            "Epoch 2/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.1253 - val_loss: 1.3164\n",
            "Epoch 3/50\n",
            "3626/3626 [==============================] - 9s 3ms/sample - loss: 1.1149 - val_loss: 1.3053\n",
            "Epoch 4/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.1151 - val_loss: 1.3155\n",
            "Epoch 5/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.1108 - val_loss: 1.3122\n",
            "Epoch 6/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.1088 - val_loss: 1.3068\n",
            "Epoch 7/50\n",
            "3626/3626 [==============================] - 9s 3ms/sample - loss: 1.1078 - val_loss: 1.2875\n",
            "Epoch 8/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.1041 - val_loss: 1.3021\n",
            "Epoch 9/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.0985 - val_loss: 1.3166\n",
            "Epoch 10/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.0999 - val_loss: 1.3007\n",
            "Epoch 11/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.0900 - val_loss: 1.3201\n",
            "Epoch 12/50\n",
            "3626/3626 [==============================] - 9s 2ms/sample - loss: 1.0933 - val_loss: 1.3171\n",
            "Drawing samples...\n",
            " MC ICP [alpha] (95%) 0.954129711751663 MSE (route) 1.9137204518057882 MAPE 0.04816716460352348 (route)\n",
            " MC ICP [sigma] (95%) 0.9698586474501109 MSE (route) 1.9137204518057882 MAPE 0.04816716460352348 (route)\n",
            "Partition 6\n",
            "Training on weeks [6 7 8 9] Fitting Linear Quantile Regression\n",
            "t + 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/regression/quantile_regression.py:192: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
            "  \") reached.\", IterationLimitWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " LR ICP (95%) 0.9170609884332281 MSE mean (route) MSE med (route) 6.848356664818052 MAPE 9.519370332446059 (route)\n",
            "Fitting model for MC Dropout\n",
            "Train on 3601 samples, validate on 951 samples\n",
            "Epoch 1/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1789 - val_loss: 6.9876\n",
            "Epoch 2/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1658 - val_loss: 7.0048\n",
            "Epoch 3/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1710 - val_loss: 6.9184\n",
            "Epoch 4/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1609 - val_loss: 6.9098\n",
            "Epoch 5/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1545 - val_loss: 6.9566\n",
            "Epoch 6/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1632 - val_loss: 6.8887\n",
            "Epoch 7/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1608 - val_loss: 6.9463\n",
            "Epoch 8/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1531 - val_loss: 7.0049\n",
            "Epoch 9/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1528 - val_loss: 6.9208\n",
            "Epoch 10/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1773 - val_loss: 6.7149\n",
            "Epoch 11/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1571 - val_loss: 6.7997\n",
            "Epoch 12/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1701 - val_loss: 6.8528\n",
            "Epoch 13/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1616 - val_loss: 6.8522\n",
            "Epoch 14/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1426 - val_loss: 6.8783\n",
            "Epoch 15/50\n",
            "3601/3601 [==============================] - 10s 3ms/sample - loss: 1.1329 - val_loss: 6.8219\n",
            "Drawing samples...\n",
            " MC ICP [alpha] (95%) 0.849763406940063 MSE (route) 9.521948500909975 MAPE 0.05584725019952898 (route)\n",
            " MC ICP [sigma] (95%) 0.9191640378548895 MSE (route) 9.521948500909975 MAPE 0.05584725019952898 (route)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEo5F683nZ5i",
        "colab_type": "code",
        "outputId": "bc031e3a-b5fa-41d0-90b0-15d06f535530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "for i in range(int(len(quantiles)/2)):\n",
        "  print(pred_ints[i])\n",
        "  print(\" LR\")\n",
        "  print(\"   ICP %.3f\" % np.mean(icp_lr[:,i],axis=1))\n",
        "  print(\"   MIL %.3f\" % (np.mean(mil_lr[:,i],axis=1)/60))\n",
        "  print(\"   ICP (route) %.3f\" % np.mean(icp2_lr[:,i],axis=1))\n",
        "  print(\"   MIL (route) %.3f\" % np.mean(mil2_lr[:,i],axis=1))\n",
        "\n",
        "  print(\" MC DROPOUT (Gal)\")\n",
        "  print(\"   ICP %.3f\" % np.mean(icp_mc_a[:,i],axis=1))\n",
        "  print(\"   MIL %.3f\" % (np.mean(mil_mc_a[:,i],axis=1)/60))\n",
        "  print(\" MC DROPOUT (Zhu)\")\n",
        "  print(\"   ICP %.5f\" % np.mean(icp_mc_s[:,i],axis=1))\n",
        "  print(\"   MIL %.5f\" % (np.mean(mil_mc_s[:,i],axis=1)/60))\n",
        "\n",
        "print(\"LR\")\n",
        "print(\"   MSE %.3f\" % np.mean(mse_lr,axis=1))\n",
        "print(\"   MAPE %.3f\" % (100*np.mean(mape_lr,axis=1)))\n",
        "print(\"   MSE (route) %.3f\" % np.mean(mse2,axis=1))\n",
        "print(\"   MAPE (route) %.3f\" % (100*np.mean(mape2_lr,axis=1)))\n",
        "print(\"MC\")\n",
        "print(\"   MSE %.3f\" % np.mean(mse_mc,axis=1))\n",
        "print(\"   MAPE %.3f\" % (100*np.mean(mape_mc,axis=1)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.95\n",
            " LR\n",
            "   ICP 0.929\n",
            "   MIL 0.914\n",
            "   ICP (route) 0.999\n",
            "   MIL (route) 877.380\n",
            " MC DROPOUT (Gal)\n",
            "   ICP 0.934\n",
            "   MIL 0.938\n",
            " MC DROPOUT (Zhu)\n",
            "   ICP 0.94187\n",
            "   MIL 0.95929\n",
            "0.9\n",
            " LR\n",
            "   ICP 0.877\n",
            "   MIL 0.727\n",
            "   ICP (route) 0.997\n",
            "   MIL (route) 698.281\n",
            " MC DROPOUT (Gal)\n",
            "   ICP 0.902\n",
            "   MIL 0.787\n",
            " MC DROPOUT (Zhu)\n",
            "   ICP 0.94194\n",
            "   MIL 0.97482\n",
            "LR\n",
            "   MSE 0.065\n",
            "   MAPE 11.548\n",
            "   MSE (route) 3.551\n",
            "   MAPE (route) 5.276\n",
            "MC\n",
            "   MSE 3.229\n",
            "   MAPE 5.103\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}